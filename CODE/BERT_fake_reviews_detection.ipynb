{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import torch.nn as nn\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel\n",
    "import torch\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        CleanedText  label\n",
      "0  love well made sturdi comfort love itveri pretti      0\n",
      "1      love great upgrad origin ive mine coupl year      0\n",
      "2            pillow save back love look feel pillow      0\n",
      "3               miss inform use great product price      0\n",
      "4               nice set good qualiti set two month      0\n"
     ]
    }
   ],
   "source": [
    "# CleanedData.csv contains the Amazon review dataset after applying Data Pre-Processing techniques\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "df = pd.read_csv(\"CleanedData.csv\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding tags to separate all the reviews\n",
    "sentences = [\"[CLS] \" + str(query) + \" [SEP]\" for query in df[\"CleanedText\"]]\n",
    "\n",
    "# Tokenize with BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the maximum sequence length. \n",
    "MAX_LEN = 334\n",
    "\n",
    "# Pad our input tokens\n",
    "input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n",
    "                          maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "\n",
    "# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create attention masks\n",
    "attention_masks = []\n",
    "\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in input_ids:\n",
    "    seq_mask = [float(i>0) for i in seq]\n",
    "    attention_masks.append(seq_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Created Binary Clssifier for BERT Model\n",
    "class BertBinaryClassifier(nn.Module):\n",
    "    def __init__(self, dropout=0.1):\n",
    "        super(BertBinaryClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(768, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, tokens, masks=None):\n",
    "        _, pooled_output = self.bert(tokens, attention_mask=masks, output_all_encoded_layers=False)\n",
    "        dropout_output = self.dropout(pooled_output)\n",
    "        linear_output = self.linear(dropout_output)\n",
    "        proba = self.sigmoid(linear_output)\n",
    "        return proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Use train_test_split to split our data into train and validation sets for training\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, df['label'].tolist(), \n",
    "                                                            random_state=2018, test_size=0.2)\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
    "                                             random_state=2018, test_size=0.2)\n",
    "                                             \n",
    "# Convert all of our data into torch tensors, the required datatype for our model\n",
    "train_inputs = torch.tensor(train_inputs).to(\"cuda\")\n",
    "validation_inputs = torch.tensor(validation_inputs).to(\"cuda\")\n",
    "train_labels = torch.tensor(train_labels).to(\"cuda\")\n",
    "validation_labels = torch.tensor(validation_labels).to(\"cuda\")\n",
    "train_masks = torch.tensor(train_masks).to(\"cuda\")\n",
    "validation_masks = torch.tensor(validation_masks).to(\"cuda\")\n",
    "\n",
    "# Select a batch size for training. \n",
    "batch_size = 16\n",
    "\n",
    "# Create an iterator of our data with torch DataLoader \n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "batch_size = 4\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [0/10]: Step [2021/505]: Loss: 0.27575:  10%|█         | 1/10 [14:05<2:06:48, 845.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8148090895037873\n",
      "32345\n",
      "448.2330109503371\n",
      "2022\n",
      "0.8807963398046247\n",
      "8087\n",
      "277.85491722418755\n",
      "2022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/10]: Step [2021/505]: Loss: 0.31505:  20%|██        | 2/10 [28:16<1:53:08, 848.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8932447055186273\n",
      "32345\n",
      "255.77579735113818\n",
      "2022\n",
      "0.8657103994064548\n",
      "8087\n",
      "315.9185113693195\n",
      "2022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [2/10]: Step [2021/505]: Loss: 0.22546:  30%|███       | 3/10 [42:19<1:38:43, 846.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9202040500850208\n",
      "32345\n",
      "193.90247065697739\n",
      "2022\n",
      "0.9025596636577222\n",
      "8087\n",
      "223.73186191160957\n",
      "2022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [3/10]: Step [2021/505]: Loss: 0.21406:  40%|████      | 4/10 [56:23<1:24:31, 845.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9370845571185654\n",
      "32345\n",
      "157.72651941046573\n",
      "2022\n",
      "0.9196240880425374\n",
      "8087\n",
      "211.54278086665767\n",
      "2022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [4/10]: Step [2021/505]: Loss: 0.20999:  50%|█████     | 5/10 [1:10:27<1:10:23, 844.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9496367290153037\n",
      "32345\n",
      "126.81499047275933\n",
      "2022\n",
      "0.9164090515642389\n",
      "8087\n",
      "208.32664317576345\n",
      "2022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [5/10]: Step [2021/505]: Loss: 0.29359:  60%|██████    | 6/10 [1:24:31<56:18, 844.56s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.960735816973257\n",
      "32345\n",
      "97.94932216724588\n",
      "2022\n",
      "0.8942747619636453\n",
      "8087\n",
      "290.2368215411389\n",
      "2022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [6/10]: Step [2021/505]: Loss: 0.23840:  70%|███████   | 7/10 [1:38:37<42:15, 845.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9702581542742309\n",
      "32345\n",
      "76.37830023657749\n",
      "2022\n",
      "0.9227154692716706\n",
      "8087\n",
      "232.82681917589827\n",
      "2022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [7/10]: Step [2021/505]: Loss: 0.24485:  80%|████████  | 8/10 [1:52:47<28:13, 846.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9761941567475653\n",
      "32345\n",
      "63.44219700339862\n",
      "2022\n",
      "0.9211079510325213\n",
      "8087\n",
      "239.93193372780462\n",
      "2022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [8/10]: Step [2021/505]: Loss: 0.22911:  90%|█████████ | 9/10 [2:06:50<14:05, 845.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9808316586798578\n",
      "32345\n",
      "50.245367743485026\n",
      "2022\n",
      "0.9298874737232595\n",
      "8087\n",
      "229.5242215063088\n",
      "2022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [9/10]: Step [2021/505]: Loss: 0.42470: 100%|██████████| 10/10 [2:20:54<00:00, 845.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9827175761323234\n",
      "32345\n",
      "44.727519656291804\n",
      "2022\n",
      "0.8983553851861011\n",
      "8087\n",
      "410.5240882475547\n",
      "2022\n"
     ]
    }
   ],
   "source": [
    "# Training the BERT Model\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 10\n",
    "\n",
    "bert_clf = BertBinaryClassifier()\n",
    "# bert_clf.cuda()\n",
    "bert_clf.to(\"cuda\")\n",
    "optimizer = torch.optim.Adam(bert_clf.parameters(), lr=3e-6)\n",
    "pbar = tqdm(range(EPOCHS))\n",
    "\n",
    "results = []\n",
    "for epoch_num in pbar:\n",
    "    bert_clf.train()\n",
    "    loss = 0\n",
    "    tes_loss = 0\n",
    "    train_acc = []\n",
    "    train_loss = []\n",
    "    test_acc = []\n",
    "    test_loss = []\n",
    "    \n",
    "    for step_num, batch_data in enumerate(train_dataloader):\n",
    "        token_ids, masks, labels = tuple(t for t in batch_data)\n",
    "        #\n",
    "        labels = labels.unsqueeze(1)\n",
    "        labels = labels.float()\n",
    "        #\n",
    "        probas = bert_clf(token_ids, masks)\n",
    "        loss_func = nn.BCELoss()\n",
    "        batch_loss = loss_func(probas, labels)\n",
    "        \n",
    "        for p, l in zip(probas, labels):\n",
    "\n",
    "        #     print(p[0].item(), l[0].item())\n",
    "            if p[0].item()>=0.5:\n",
    "                if l[0] == 1:\n",
    "                    train_acc += [1]\n",
    "                else:\n",
    "                    train_acc += [0]\n",
    "            else:\n",
    "                if l[0] == 0:\n",
    "                    train_acc += [1]\n",
    "                else:\n",
    "                    train_acc += [0]\n",
    "        \n",
    "        loss += batch_loss.item()\n",
    "        train_loss += [loss]\n",
    "        \n",
    "        bert_clf.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "#         print(train_loss)\n",
    "\n",
    "        pbar.set_description(\"Epoch [%d/%d]: Step [%d/%d]: Loss: %.5f\" % (epoch_num, EPOCHS, step_num, int(len(train_data)/BATCH_SIZE), loss/(step_num+1)))\n",
    "\n",
    "    bert_clf.eval()\n",
    "    for step_num, batch_data in enumerate(validation_dataloader):\n",
    "        token_ids, masks, labels = tuple(t for t in batch_data)\n",
    "        #\n",
    "        labels = labels.unsqueeze(1)\n",
    "        labels = labels.float()\n",
    "        #\n",
    "        probas = bert_clf(token_ids, masks)\n",
    "        loss_func = nn.BCELoss()\n",
    "        batch_loss = loss_func(probas, labels)\n",
    "        \n",
    "        for p, l in zip(probas, labels):\n",
    "\n",
    "            if p[0].item()>=0.5:\n",
    "                if l[0] == 1:\n",
    "                    test_acc += [1]\n",
    "                else:\n",
    "                    test_acc += [0]\n",
    "            else:\n",
    "                if l[0] == 0:\n",
    "                    test_acc += [1]\n",
    "                else:\n",
    "                    test_acc += [0]\n",
    "        \n",
    "        tes_loss += batch_loss.item()\n",
    "        test_loss += [tes_loss]\n",
    "        \n",
    "        pbar.set_description(\"Epoch [%d/%d]: Step [%d/%d]: Loss: %.5f\" % (epoch_num, EPOCHS, step_num, int(len(validation_data)/BATCH_SIZE), tes_loss/(step_num+1)))\n",
    "    \n",
    "    print(np.mean(train_acc))\n",
    "    print(len(train_acc))\n",
    "    print(np.mean(train_loss))\n",
    "    print(len(train_loss))\n",
    "    print(np.mean(test_acc))\n",
    "    print(len(test_acc))\n",
    "    print(np.mean(test_loss))\n",
    "    print(len(test_loss))\n",
    "    results.append([np.mean(train_loss), np.mean(train_acc), np.mean(test_loss), np.mean(test_acc)])\n",
    "    \n",
    "df = pd.DataFrame(results, columns =[\"Train Loss\", \"Train Accuracy\", \"Test Loss\", \"Test Accuracy\"])\n",
    "df.to_csv(\"bert_clf.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained BERT Model\n",
    "torch.save(bert_clf.state_dict(), 'bert_clf.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step [0/505]: Loss: 0.06601\n",
      "Step [1/505]: Loss: 0.03360\n",
      "Step [2/505]: Loss: 0.02245\n",
      "Step [3/505]: Loss: 0.01687\n",
      "Step [4/505]: Loss: 0.02741\n",
      "Step [5/505]: Loss: 0.02311\n",
      "Step [6/505]: Loss: 0.05494\n",
      "Step [7/505]: Loss: 0.04810\n",
      "Step [8/505]: Loss: 0.04346\n",
      "Step [9/505]: Loss: 0.11236\n",
      "Step [10/505]: Loss: 0.11641\n",
      "Step [11/505]: Loss: 0.11818\n",
      "Step [12/505]: Loss: 0.10915\n",
      "Step [13/505]: Loss: 0.10457\n",
      "Step [14/505]: Loss: 0.10568\n",
      "Step [15/505]: Loss: 0.09911\n",
      "Step [16/505]: Loss: 0.15976\n",
      "Step [17/505]: Loss: 0.23658\n",
      "Step [18/505]: Loss: 0.22414\n",
      "Step [19/505]: Loss: 0.21295\n",
      "Step [20/505]: Loss: 0.31048\n",
      "Step [21/505]: Loss: 0.29638\n",
      "Step [22/505]: Loss: 0.36115\n",
      "Step [23/505]: Loss: 0.34617\n",
      "Step [24/505]: Loss: 0.33831\n",
      "Step [25/505]: Loss: 0.32536\n",
      "Step [26/505]: Loss: 0.31332\n",
      "Step [27/505]: Loss: 0.30214\n",
      "Step [28/505]: Loss: 0.29496\n",
      "Step [29/505]: Loss: 0.28514\n",
      "Step [30/505]: Loss: 0.27596\n",
      "Step [31/505]: Loss: 0.26737\n",
      "Step [32/505]: Loss: 0.26379\n",
      "Step [33/505]: Loss: 0.25612\n",
      "Step [34/505]: Loss: 0.26623\n",
      "Step [35/505]: Loss: 0.25926\n",
      "Step [36/505]: Loss: 0.25328\n",
      "Step [37/505]: Loss: 0.24662\n",
      "Step [38/505]: Loss: 0.25374\n",
      "Step [39/505]: Loss: 0.25543\n",
      "Step [40/505]: Loss: 0.26176\n",
      "Step [41/505]: Loss: 0.25790\n",
      "Step [42/505]: Loss: 0.27631\n",
      "Step [43/505]: Loss: 0.27944\n",
      "Step [44/505]: Loss: 0.28042\n",
      "Step [45/505]: Loss: 0.27432\n",
      "Step [46/505]: Loss: 0.26855\n",
      "Step [47/505]: Loss: 0.26429\n",
      "Step [48/505]: Loss: 0.25960\n",
      "Step [49/505]: Loss: 0.25441\n",
      "Step [50/505]: Loss: 0.24943\n",
      "Step [51/505]: Loss: 0.24583\n",
      "Step [52/505]: Loss: 0.24119\n",
      "Step [53/505]: Loss: 0.27796\n",
      "Step [54/505]: Loss: 0.27292\n",
      "Step [55/505]: Loss: 0.26805\n",
      "Step [56/505]: Loss: 0.26335\n",
      "Step [57/505]: Loss: 0.28598\n",
      "Step [58/505]: Loss: 0.28116\n",
      "Step [59/505]: Loss: 0.30087\n",
      "Step [60/505]: Loss: 0.30144\n",
      "Step [61/505]: Loss: 0.29659\n",
      "Step [62/505]: Loss: 0.29188\n",
      "Step [63/505]: Loss: 0.28732\n",
      "Step [64/505]: Loss: 0.28605\n",
      "Step [65/505]: Loss: 0.28246\n",
      "Step [66/505]: Loss: 0.27828\n",
      "Step [67/505]: Loss: 0.30304\n",
      "Step [68/505]: Loss: 0.29865\n",
      "Step [69/505]: Loss: 0.29487\n",
      "Step [70/505]: Loss: 0.29074\n",
      "Step [71/505]: Loss: 0.28693\n",
      "Step [72/505]: Loss: 0.28533\n",
      "Step [73/505]: Loss: 0.28149\n",
      "Step [74/505]: Loss: 0.27781\n",
      "Step [75/505]: Loss: 0.27429\n",
      "Step [76/505]: Loss: 0.27073\n",
      "Step [77/505]: Loss: 0.27866\n",
      "Step [78/505]: Loss: 0.27594\n",
      "Step [79/505]: Loss: 0.27252\n",
      "Step [80/505]: Loss: 0.27200\n",
      "Step [81/505]: Loss: 0.27211\n",
      "Step [82/505]: Loss: 0.26883\n",
      "Step [83/505]: Loss: 0.26660\n",
      "Step [84/505]: Loss: 0.26347\n",
      "Step [85/505]: Loss: 0.26041\n",
      "Step [86/505]: Loss: 0.29808\n",
      "Step [87/505]: Loss: 0.29484\n",
      "Step [88/505]: Loss: 0.29219\n",
      "Step [89/505]: Loss: 0.29396\n",
      "Step [90/505]: Loss: 0.29081\n",
      "Step [91/505]: Loss: 0.28771\n",
      "Step [92/505]: Loss: 0.28469\n",
      "Step [93/505]: Loss: 0.28167\n",
      "Step [94/505]: Loss: 0.27931\n",
      "Step [95/505]: Loss: 0.27777\n",
      "Step [96/505]: Loss: 0.27492\n",
      "Step [97/505]: Loss: 0.27212\n",
      "Step [98/505]: Loss: 0.26937\n",
      "Step [99/505]: Loss: 0.26668\n",
      "Step [100/505]: Loss: 0.27831\n",
      "Step [101/505]: Loss: 0.27565\n",
      "Step [102/505]: Loss: 0.27385\n",
      "Step [103/505]: Loss: 0.27122\n",
      "Step [104/505]: Loss: 0.26864\n",
      "Step [105/505]: Loss: 0.26666\n",
      "Step [106/505]: Loss: 0.26664\n",
      "Step [107/505]: Loss: 0.26448\n",
      "Step [108/505]: Loss: 0.27518\n",
      "Step [109/505]: Loss: 0.27291\n",
      "Step [110/505]: Loss: 0.27841\n",
      "Step [111/505]: Loss: 0.27593\n",
      "Step [112/505]: Loss: 0.27432\n",
      "Step [113/505]: Loss: 0.28203\n",
      "Step [114/505]: Loss: 0.27997\n",
      "Step [115/505]: Loss: 0.28872\n",
      "Step [116/505]: Loss: 0.29833\n",
      "Step [117/505]: Loss: 0.29581\n",
      "Step [118/505]: Loss: 0.29479\n",
      "Step [119/505]: Loss: 0.29328\n",
      "Step [120/505]: Loss: 0.29564\n",
      "Step [121/505]: Loss: 0.29406\n",
      "Step [122/505]: Loss: 0.29167\n",
      "Step [123/505]: Loss: 0.28935\n",
      "Step [124/505]: Loss: 0.28965\n",
      "Step [125/505]: Loss: 0.28736\n",
      "Step [126/505]: Loss: 0.28513\n",
      "Step [127/505]: Loss: 0.28293\n",
      "Step [128/505]: Loss: 0.28358\n",
      "Step [129/505]: Loss: 0.28171\n",
      "Step [130/505]: Loss: 0.28457\n",
      "Step [131/505]: Loss: 0.28244\n",
      "Step [132/505]: Loss: 0.29622\n",
      "Step [133/505]: Loss: 0.29408\n",
      "Step [134/505]: Loss: 0.29191\n",
      "Step [135/505]: Loss: 0.29038\n",
      "Step [136/505]: Loss: 0.28830\n",
      "Step [137/505]: Loss: 0.28630\n",
      "Step [138/505]: Loss: 0.28425\n",
      "Step [139/505]: Loss: 0.28457\n",
      "Step [140/505]: Loss: 0.28255\n",
      "Step [141/505]: Loss: 0.28056\n",
      "Step [142/505]: Loss: 0.28928\n",
      "Step [143/505]: Loss: 0.28728\n",
      "Step [144/505]: Loss: 0.28766\n",
      "Step [145/505]: Loss: 0.28576\n",
      "Step [146/505]: Loss: 0.28382\n",
      "Step [147/505]: Loss: 0.28191\n",
      "Step [148/505]: Loss: 0.28005\n",
      "Step [149/505]: Loss: 0.27819\n",
      "Step [150/505]: Loss: 0.27649\n",
      "Step [151/505]: Loss: 0.27495\n",
      "Step [152/505]: Loss: 0.27315\n",
      "Step [153/505]: Loss: 0.27138\n",
      "Step [154/505]: Loss: 0.27011\n",
      "Step [155/505]: Loss: 0.26840\n",
      "Step [156/505]: Loss: 0.27225\n",
      "Step [157/505]: Loss: 0.27067\n",
      "Step [158/505]: Loss: 0.26897\n",
      "Step [159/505]: Loss: 0.27672\n",
      "Step [160/505]: Loss: 0.27500\n",
      "Step [161/505]: Loss: 0.27331\n",
      "Step [162/505]: Loss: 0.27164\n",
      "Step [163/505]: Loss: 0.27631\n",
      "Step [164/505]: Loss: 0.27647\n",
      "Step [165/505]: Loss: 0.27481\n",
      "Step [166/505]: Loss: 0.27317\n",
      "Step [167/505]: Loss: 0.27154\n",
      "Step [168/505]: Loss: 0.26994\n",
      "Step [169/505]: Loss: 0.26835\n",
      "Step [170/505]: Loss: 0.26751\n",
      "Step [171/505]: Loss: 0.26611\n",
      "Step [172/505]: Loss: 0.26457\n",
      "Step [173/505]: Loss: 0.26305\n",
      "Step [174/505]: Loss: 0.26165\n",
      "Step [175/505]: Loss: 0.26044\n",
      "Step [176/505]: Loss: 0.25897\n",
      "Step [177/505]: Loss: 0.25755\n",
      "Step [178/505]: Loss: 0.25622\n",
      "Step [179/505]: Loss: 0.25543\n",
      "Step [180/505]: Loss: 0.25410\n",
      "Step [181/505]: Loss: 0.25857\n",
      "Step [182/505]: Loss: 0.25771\n",
      "Step [183/505]: Loss: 0.25632\n",
      "Step [184/505]: Loss: 0.25493\n",
      "Step [185/505]: Loss: 0.25356\n",
      "Step [186/505]: Loss: 0.25229\n",
      "Step [187/505]: Loss: 0.25120\n",
      "Step [188/505]: Loss: 0.24987\n",
      "Step [189/505]: Loss: 0.24857\n",
      "Step [190/505]: Loss: 0.24731\n",
      "Step [191/505]: Loss: 0.24733\n",
      "Step [192/505]: Loss: 0.24605\n",
      "Step [193/505]: Loss: 0.24763\n",
      "Step [194/505]: Loss: 0.24637\n",
      "Step [195/505]: Loss: 0.24511\n",
      "Step [196/505]: Loss: 0.24994\n",
      "Step [197/505]: Loss: 0.24887\n",
      "Step [198/505]: Loss: 0.24821\n",
      "Step [199/505]: Loss: 0.25599\n",
      "Step [200/505]: Loss: 0.25723\n",
      "Step [201/505]: Loss: 0.25741\n",
      "Step [202/505]: Loss: 0.25614\n",
      "Step [203/505]: Loss: 0.25521\n",
      "Step [204/505]: Loss: 0.25397\n",
      "Step [205/505]: Loss: 0.25274\n",
      "Step [206/505]: Loss: 0.25380\n",
      "Step [207/505]: Loss: 0.25376\n",
      "Step [208/505]: Loss: 0.25255\n",
      "Step [209/505]: Loss: 0.25136\n",
      "Step [210/505]: Loss: 0.25020\n",
      "Step [211/505]: Loss: 0.24902\n",
      "Step [212/505]: Loss: 0.24786\n",
      "Step [213/505]: Loss: 0.24670\n",
      "Step [214/505]: Loss: 0.24556\n",
      "Step [215/505]: Loss: 0.24455\n",
      "Step [216/505]: Loss: 0.24343\n",
      "Step [217/505]: Loss: 0.24235\n",
      "Step [218/505]: Loss: 0.24124\n",
      "Step [219/505]: Loss: 0.24015\n",
      "Step [220/505]: Loss: 0.24279\n",
      "Step [221/505]: Loss: 0.24271\n",
      "Step [222/505]: Loss: 0.24162\n",
      "Step [223/505]: Loss: 0.24054\n",
      "Step [224/505]: Loss: 0.23958\n",
      "Step [225/505]: Loss: 0.23852\n",
      "Step [226/505]: Loss: 0.23760\n",
      "Step [227/505]: Loss: 0.23893\n",
      "Step [228/505]: Loss: 0.23789\n",
      "Step [229/505]: Loss: 0.23686\n",
      "Step [230/505]: Loss: 0.23584\n",
      "Step [231/505]: Loss: 0.23487\n",
      "Step [232/505]: Loss: 0.23387\n",
      "Step [233/505]: Loss: 0.23748\n",
      "Step [234/505]: Loss: 0.23648\n",
      "Step [235/505]: Loss: 0.23549\n",
      "Step [236/505]: Loss: 0.23450\n",
      "Step [237/505]: Loss: 0.23469\n",
      "Step [238/505]: Loss: 0.23980\n",
      "Step [239/505]: Loss: 0.23881\n",
      "Step [240/505]: Loss: 0.23782\n",
      "Step [241/505]: Loss: 0.23684\n",
      "Step [242/505]: Loss: 0.23587\n",
      "Step [243/505]: Loss: 0.23492\n",
      "Step [244/505]: Loss: 0.23397\n",
      "Step [245/505]: Loss: 0.23304\n",
      "Step [246/505]: Loss: 0.23210\n",
      "Step [247/505]: Loss: 0.23117\n",
      "Step [248/505]: Loss: 0.23027\n",
      "Step [249/505]: Loss: 0.22935\n",
      "Step [250/505]: Loss: 0.22844\n",
      "Step [251/505]: Loss: 0.22775\n",
      "Step [252/505]: Loss: 0.22696\n",
      "Step [253/505]: Loss: 0.22607\n",
      "Step [254/505]: Loss: 0.22535\n",
      "Step [255/505]: Loss: 0.22449\n",
      "Step [256/505]: Loss: 0.22362\n",
      "Step [257/505]: Loss: 0.22275\n",
      "Step [258/505]: Loss: 0.22990\n",
      "Step [259/505]: Loss: 0.22902\n",
      "Step [260/505]: Loss: 0.22815\n",
      "Step [261/505]: Loss: 0.22729\n",
      "Step [262/505]: Loss: 0.22648\n",
      "Step [263/505]: Loss: 0.22660\n",
      "Step [264/505]: Loss: 0.22575\n",
      "Step [265/505]: Loss: 0.22490\n",
      "Step [266/505]: Loss: 0.22406\n",
      "Step [267/505]: Loss: 0.22353\n",
      "Step [268/505]: Loss: 0.22536\n",
      "Step [269/505]: Loss: 0.22458\n",
      "Step [270/505]: Loss: 0.22524\n",
      "Step [271/505]: Loss: 0.22493\n",
      "Step [272/505]: Loss: 0.22602\n",
      "Step [273/505]: Loss: 0.22528\n",
      "Step [274/505]: Loss: 0.22446\n",
      "Step [275/505]: Loss: 0.23291\n",
      "Step [276/505]: Loss: 0.23209\n",
      "Step [277/505]: Loss: 0.23314\n",
      "Step [278/505]: Loss: 0.23231\n",
      "Step [279/505]: Loss: 0.23150\n",
      "Step [280/505]: Loss: 0.23067\n",
      "Step [281/505]: Loss: 0.23027\n",
      "Step [282/505]: Loss: 0.22947\n",
      "Step [283/505]: Loss: 0.22866\n",
      "Step [284/505]: Loss: 0.22799\n",
      "Step [285/505]: Loss: 0.22760\n",
      "Step [286/505]: Loss: 0.23011\n",
      "Step [287/505]: Loss: 0.23168\n",
      "Step [288/505]: Loss: 0.23090\n",
      "Step [289/505]: Loss: 0.23012\n",
      "Step [290/505]: Loss: 0.22933\n",
      "Step [291/505]: Loss: 0.22854\n",
      "Step [292/505]: Loss: 0.23012\n",
      "Step [293/505]: Loss: 0.24027\n",
      "Step [294/505]: Loss: 0.23948\n",
      "Step [295/505]: Loss: 0.23871\n",
      "Step [296/505]: Loss: 0.24217\n",
      "Step [297/505]: Loss: 0.24136\n",
      "Step [298/505]: Loss: 0.24145\n",
      "Step [299/505]: Loss: 0.24065\n",
      "Step [300/505]: Loss: 0.24012\n",
      "Step [301/505]: Loss: 0.23947\n",
      "Step [302/505]: Loss: 0.23869\n",
      "Step [303/505]: Loss: 0.23956\n",
      "Step [304/505]: Loss: 0.24436\n",
      "Step [305/505]: Loss: 0.24420\n",
      "Step [306/505]: Loss: 0.24342\n",
      "Step [307/505]: Loss: 0.24304\n",
      "Step [308/505]: Loss: 0.24250\n",
      "Step [309/505]: Loss: 0.24173\n",
      "Step [310/505]: Loss: 0.24099\n",
      "Step [311/505]: Loss: 0.24022\n",
      "Step [312/505]: Loss: 0.23950\n",
      "Step [313/505]: Loss: 0.23895\n",
      "Step [314/505]: Loss: 0.23819\n",
      "Step [315/505]: Loss: 0.23752\n",
      "Step [316/505]: Loss: 0.23794\n",
      "Step [317/505]: Loss: 0.23745\n",
      "Step [318/505]: Loss: 0.23671\n",
      "Step [319/505]: Loss: 0.23613\n",
      "Step [320/505]: Loss: 0.23539\n",
      "Step [321/505]: Loss: 0.23673\n",
      "Step [322/505]: Loss: 0.23617\n",
      "Step [323/505]: Loss: 0.23544\n",
      "Step [324/505]: Loss: 0.23562\n",
      "Step [325/505]: Loss: 0.23491\n",
      "Step [326/505]: Loss: 0.23438\n",
      "Step [327/505]: Loss: 0.23366\n",
      "Step [328/505]: Loss: 0.23295\n",
      "Step [329/505]: Loss: 0.23225\n",
      "Step [330/505]: Loss: 0.23155\n",
      "Step [331/505]: Loss: 0.23346\n",
      "Step [332/505]: Loss: 0.23322\n",
      "Step [333/505]: Loss: 0.23709\n",
      "Step [334/505]: Loss: 0.23638\n",
      "Step [335/505]: Loss: 0.23568\n",
      "Step [336/505]: Loss: 0.23604\n",
      "Step [337/505]: Loss: 0.23795\n",
      "Step [338/505]: Loss: 0.23725\n",
      "Step [339/505]: Loss: 0.23655\n",
      "Step [340/505]: Loss: 0.23586\n",
      "Step [341/505]: Loss: 0.23518\n",
      "Step [342/505]: Loss: 0.23502\n",
      "Step [343/505]: Loss: 0.23715\n",
      "Step [344/505]: Loss: 0.23647\n",
      "Step [345/505]: Loss: 0.23578\n",
      "Step [346/505]: Loss: 0.23512\n",
      "Step [347/505]: Loss: 0.23481\n",
      "Step [348/505]: Loss: 0.23414\n",
      "Step [349/505]: Loss: 0.23348\n",
      "Step [350/505]: Loss: 0.23281\n",
      "Step [351/505]: Loss: 0.23215\n",
      "Step [352/505]: Loss: 0.23210\n",
      "Step [353/505]: Loss: 0.23145\n",
      "Step [354/505]: Loss: 0.23125\n",
      "Step [355/505]: Loss: 0.23085\n",
      "Step [356/505]: Loss: 0.23034\n",
      "Step [357/505]: Loss: 0.22997\n",
      "Step [358/505]: Loss: 0.22933\n",
      "Step [359/505]: Loss: 0.22870\n",
      "Step [360/505]: Loss: 0.22807\n",
      "Step [361/505]: Loss: 0.22816\n",
      "Step [362/505]: Loss: 0.23110\n",
      "Step [363/505]: Loss: 0.23106\n",
      "Step [364/505]: Loss: 0.23043\n",
      "Step [365/505]: Loss: 0.22980\n",
      "Step [366/505]: Loss: 0.23291\n",
      "Step [367/505]: Loss: 0.23304\n",
      "Step [368/505]: Loss: 0.23241\n",
      "Step [369/505]: Loss: 0.23178\n",
      "Step [370/505]: Loss: 0.23148\n",
      "Step [371/505]: Loss: 0.23086\n",
      "Step [372/505]: Loss: 0.23383\n",
      "Step [373/505]: Loss: 0.23321\n",
      "Step [374/505]: Loss: 0.23259\n",
      "Step [375/505]: Loss: 0.23199\n",
      "Step [376/505]: Loss: 0.23137\n",
      "Step [377/505]: Loss: 0.23076\n",
      "Step [378/505]: Loss: 0.23028\n",
      "Step [379/505]: Loss: 0.23421\n",
      "Step [380/505]: Loss: 0.23360\n",
      "Step [381/505]: Loss: 0.23299\n",
      "Step [382/505]: Loss: 0.23242\n",
      "Step [383/505]: Loss: 0.23579\n",
      "Step [384/505]: Loss: 0.23518\n",
      "Step [385/505]: Loss: 0.23457\n",
      "Step [386/505]: Loss: 0.23696\n",
      "Step [387/505]: Loss: 0.23635\n",
      "Step [388/505]: Loss: 0.23574\n",
      "Step [389/505]: Loss: 0.23514\n",
      "Step [390/505]: Loss: 0.23611\n",
      "Step [391/505]: Loss: 0.23553\n",
      "Step [392/505]: Loss: 0.23620\n",
      "Step [393/505]: Loss: 0.23569\n",
      "Step [394/505]: Loss: 0.23511\n",
      "Step [395/505]: Loss: 0.23853\n",
      "Step [396/505]: Loss: 0.23793\n",
      "Step [397/505]: Loss: 0.23734\n",
      "Step [398/505]: Loss: 0.23680\n",
      "Step [399/505]: Loss: 0.24163\n",
      "Step [400/505]: Loss: 0.24104\n",
      "Step [401/505]: Loss: 0.24044\n",
      "Step [402/505]: Loss: 0.23996\n",
      "Step [403/505]: Loss: 0.23936\n",
      "Step [404/505]: Loss: 0.24221\n",
      "Step [405/505]: Loss: 0.24162\n",
      "Step [406/505]: Loss: 0.24105\n",
      "Step [407/505]: Loss: 0.24046\n",
      "Step [408/505]: Loss: 0.23988\n",
      "Step [409/505]: Loss: 0.23930\n",
      "Step [410/505]: Loss: 0.23872\n",
      "Step [411/505]: Loss: 0.23948\n",
      "Step [412/505]: Loss: 0.23979\n",
      "Step [413/505]: Loss: 0.23921\n",
      "Step [414/505]: Loss: 0.24307\n",
      "Step [415/505]: Loss: 0.24248\n",
      "Step [416/505]: Loss: 0.24190\n",
      "Step [417/505]: Loss: 0.24434\n",
      "Step [418/505]: Loss: 0.24603\n",
      "Step [419/505]: Loss: 0.24544\n",
      "Step [420/505]: Loss: 0.24490\n",
      "Step [421/505]: Loss: 0.24432\n",
      "Step [422/505]: Loss: 0.24374\n",
      "Step [423/505]: Loss: 0.24324\n",
      "Step [424/505]: Loss: 0.24267\n",
      "Step [425/505]: Loss: 0.24304\n",
      "Step [426/505]: Loss: 0.24248\n",
      "Step [427/505]: Loss: 0.24325\n",
      "Step [428/505]: Loss: 0.24269\n",
      "Step [429/505]: Loss: 0.24443\n",
      "Step [430/505]: Loss: 0.24386\n",
      "Step [431/505]: Loss: 0.24330\n",
      "Step [432/505]: Loss: 0.24274\n",
      "Step [433/505]: Loss: 0.24218\n",
      "Step [434/505]: Loss: 0.24163\n",
      "Step [435/505]: Loss: 0.24108\n",
      "Step [436/505]: Loss: 0.24053\n",
      "Step [437/505]: Loss: 0.24415\n",
      "Step [438/505]: Loss: 0.24363\n",
      "Step [439/505]: Loss: 0.24328\n",
      "Step [440/505]: Loss: 0.24273\n",
      "Step [441/505]: Loss: 0.24296\n",
      "Step [442/505]: Loss: 0.24248\n",
      "Step [443/505]: Loss: 0.24195\n",
      "Step [444/505]: Loss: 0.24140\n",
      "Step [445/505]: Loss: 0.24086\n",
      "Step [446/505]: Loss: 0.24035\n",
      "Step [447/505]: Loss: 0.23981\n",
      "Step [448/505]: Loss: 0.23928\n",
      "Step [449/505]: Loss: 0.23875\n",
      "Step [450/505]: Loss: 0.23822\n",
      "Step [451/505]: Loss: 0.23769\n",
      "Step [452/505]: Loss: 0.23717\n",
      "Step [453/505]: Loss: 0.23665\n",
      "Step [454/505]: Loss: 0.23613\n",
      "Step [455/505]: Loss: 0.23561\n",
      "Step [456/505]: Loss: 0.23510\n",
      "Step [457/505]: Loss: 0.23460\n",
      "Step [458/505]: Loss: 0.23408\n",
      "Step [459/505]: Loss: 0.23360\n",
      "Step [460/505]: Loss: 0.23310\n",
      "Step [461/505]: Loss: 0.23261\n",
      "Step [462/505]: Loss: 0.23221\n",
      "Step [463/505]: Loss: 0.23171\n",
      "Step [464/505]: Loss: 0.23123\n",
      "Step [465/505]: Loss: 0.23082\n",
      "Step [466/505]: Loss: 0.23033\n",
      "Step [467/505]: Loss: 0.22983\n",
      "Step [468/505]: Loss: 0.22934\n",
      "Step [469/505]: Loss: 0.22886\n",
      "Step [470/505]: Loss: 0.22837\n",
      "Step [471/505]: Loss: 0.22789\n",
      "Step [472/505]: Loss: 0.22741\n",
      "Step [473/505]: Loss: 0.22693\n",
      "Step [474/505]: Loss: 0.22648\n",
      "Step [475/505]: Loss: 0.22600\n",
      "Step [476/505]: Loss: 0.22733\n",
      "Step [477/505]: Loss: 0.22685\n",
      "Step [478/505]: Loss: 0.22641\n",
      "Step [479/505]: Loss: 0.22594\n",
      "Step [480/505]: Loss: 0.22549\n",
      "Step [481/505]: Loss: 0.22503\n",
      "Step [482/505]: Loss: 0.22566\n",
      "Step [483/505]: Loss: 0.22607\n",
      "Step [484/505]: Loss: 0.22561\n",
      "Step [485/505]: Loss: 0.22515\n",
      "Step [486/505]: Loss: 0.22469\n",
      "Step [487/505]: Loss: 0.22524\n",
      "Step [488/505]: Loss: 0.22478\n",
      "Step [489/505]: Loss: 0.22433\n",
      "Step [490/505]: Loss: 0.22398\n",
      "Step [491/505]: Loss: 0.22353\n",
      "Step [492/505]: Loss: 0.22357\n",
      "Step [493/505]: Loss: 0.22317\n",
      "Step [494/505]: Loss: 0.22276\n",
      "Step [495/505]: Loss: 0.22231\n",
      "Step [496/505]: Loss: 0.22186\n",
      "Step [497/505]: Loss: 0.22213\n",
      "Step [498/505]: Loss: 0.22694\n",
      "Step [499/505]: Loss: 0.22819\n",
      "Step [500/505]: Loss: 0.22787\n",
      "Step [501/505]: Loss: 0.22742\n",
      "Step [502/505]: Loss: 0.22697\n",
      "Step [503/505]: Loss: 0.22652\n",
      "Step [504/505]: Loss: 0.22608\n",
      "Step [505/505]: Loss: 0.22596\n",
      "Step [506/505]: Loss: 0.22562\n",
      "Step [507/505]: Loss: 0.22928\n",
      "Step [508/505]: Loss: 0.22883\n",
      "Step [509/505]: Loss: 0.23245\n",
      "Step [510/505]: Loss: 0.23202\n",
      "Step [511/505]: Loss: 0.23201\n",
      "Step [512/505]: Loss: 0.23156\n",
      "Step [513/505]: Loss: 0.23137\n",
      "Step [514/505]: Loss: 0.23102\n",
      "Step [515/505]: Loss: 0.23250\n",
      "Step [516/505]: Loss: 0.23205\n",
      "Step [517/505]: Loss: 0.23388\n",
      "Step [518/505]: Loss: 0.23450\n",
      "Step [519/505]: Loss: 0.23405\n",
      "Step [520/505]: Loss: 0.23898\n",
      "Step [521/505]: Loss: 0.23852\n",
      "Step [522/505]: Loss: 0.23819\n",
      "Step [523/505]: Loss: 0.23774\n",
      "Step [524/505]: Loss: 0.23894\n",
      "Step [525/505]: Loss: 0.23849\n",
      "Step [526/505]: Loss: 0.23809\n",
      "Step [527/505]: Loss: 0.23933\n",
      "Step [528/505]: Loss: 0.23888\n",
      "Step [529/505]: Loss: 0.23843\n",
      "Step [530/505]: Loss: 0.23920\n",
      "Step [531/505]: Loss: 0.23879\n",
      "Step [532/505]: Loss: 0.23835\n",
      "Step [533/505]: Loss: 0.23790\n",
      "Step [534/505]: Loss: 0.23746\n",
      "Step [535/505]: Loss: 0.23706\n",
      "Step [536/505]: Loss: 0.23675\n",
      "Step [537/505]: Loss: 0.23631\n",
      "Step [538/505]: Loss: 0.23590\n",
      "Step [539/505]: Loss: 0.23546\n",
      "Step [540/505]: Loss: 0.23509\n",
      "Step [541/505]: Loss: 0.23465\n",
      "Step [542/505]: Loss: 0.23711\n",
      "Step [543/505]: Loss: 0.23667\n",
      "Step [544/505]: Loss: 0.23624\n",
      "Step [545/505]: Loss: 0.23583\n",
      "Step [546/505]: Loss: 0.23894\n",
      "Step [547/505]: Loss: 0.23854\n",
      "Step [548/505]: Loss: 0.23812\n",
      "Step [549/505]: Loss: 0.23769\n",
      "Step [550/505]: Loss: 0.23725\n",
      "Step [551/505]: Loss: 0.23683\n",
      "Step [552/505]: Loss: 0.23857\n",
      "Step [553/505]: Loss: 0.23814\n",
      "Step [554/505]: Loss: 0.23807\n",
      "Step [555/505]: Loss: 0.23765\n",
      "Step [556/505]: Loss: 0.24112\n",
      "Step [557/505]: Loss: 0.24069\n",
      "Step [558/505]: Loss: 0.24029\n",
      "Step [559/505]: Loss: 0.23987\n",
      "Step [560/505]: Loss: 0.24263\n",
      "Step [561/505]: Loss: 0.24239\n",
      "Step [562/505]: Loss: 0.24199\n",
      "Step [563/505]: Loss: 0.24156\n",
      "Step [564/505]: Loss: 0.24235\n",
      "Step [565/505]: Loss: 0.24192\n",
      "Step [566/505]: Loss: 0.24150\n",
      "Step [567/505]: Loss: 0.24476\n",
      "Step [568/505]: Loss: 0.24433\n",
      "Step [569/505]: Loss: 0.24440\n",
      "Step [570/505]: Loss: 0.24398\n",
      "Step [571/505]: Loss: 0.24356\n",
      "Step [572/505]: Loss: 0.24314\n",
      "Step [573/505]: Loss: 0.24271\n",
      "Step [574/505]: Loss: 0.24231\n",
      "Step [575/505]: Loss: 0.24189\n",
      "Step [576/505]: Loss: 0.24279\n",
      "Step [577/505]: Loss: 0.24237\n",
      "Step [578/505]: Loss: 0.24198\n",
      "Step [579/505]: Loss: 0.24157\n",
      "Step [580/505]: Loss: 0.24241\n",
      "Step [581/505]: Loss: 0.24200\n",
      "Step [582/505]: Loss: 0.24327\n",
      "Step [583/505]: Loss: 0.24510\n",
      "Step [584/505]: Loss: 0.24472\n",
      "Step [585/505]: Loss: 0.24702\n",
      "Step [586/505]: Loss: 0.24663\n",
      "Step [587/505]: Loss: 0.24621\n",
      "Step [588/505]: Loss: 0.24872\n",
      "Step [589/505]: Loss: 0.24830\n",
      "Step [590/505]: Loss: 0.24791\n",
      "Step [591/505]: Loss: 0.24751\n",
      "Step [592/505]: Loss: 0.24710\n",
      "Step [593/505]: Loss: 0.24684\n",
      "Step [594/505]: Loss: 0.24642\n",
      "Step [595/505]: Loss: 0.24617\n",
      "Step [596/505]: Loss: 0.24581\n",
      "Step [597/505]: Loss: 0.24540\n",
      "Step [598/505]: Loss: 0.24499\n",
      "Step [599/505]: Loss: 0.24665\n",
      "Step [600/505]: Loss: 0.24624\n",
      "Step [601/505]: Loss: 0.24584\n",
      "Step [602/505]: Loss: 0.24548\n",
      "Step [603/505]: Loss: 0.24507\n",
      "Step [604/505]: Loss: 0.24467\n",
      "Step [605/505]: Loss: 0.24427\n",
      "Step [606/505]: Loss: 0.24387\n",
      "Step [607/505]: Loss: 0.24347\n",
      "Step [608/505]: Loss: 0.24313\n",
      "Step [609/505]: Loss: 0.24422\n",
      "Step [610/505]: Loss: 0.24382\n",
      "Step [611/505]: Loss: 0.24342\n",
      "Step [612/505]: Loss: 0.24398\n",
      "Step [613/505]: Loss: 0.24359\n",
      "Step [614/505]: Loss: 0.24322\n",
      "Step [615/505]: Loss: 0.24498\n",
      "Step [616/505]: Loss: 0.24458\n",
      "Step [617/505]: Loss: 0.24421\n",
      "Step [618/505]: Loss: 0.24511\n",
      "Step [619/505]: Loss: 0.24472\n",
      "Step [620/505]: Loss: 0.24434\n",
      "Step [621/505]: Loss: 0.24398\n",
      "Step [622/505]: Loss: 0.24370\n",
      "Step [623/505]: Loss: 0.24331\n",
      "Step [624/505]: Loss: 0.24380\n",
      "Step [625/505]: Loss: 0.24341\n",
      "Step [626/505]: Loss: 0.24308\n",
      "Step [627/505]: Loss: 0.24447\n",
      "Step [628/505]: Loss: 0.24415\n",
      "Step [629/505]: Loss: 0.24422\n",
      "Step [630/505]: Loss: 0.24383\n",
      "Step [631/505]: Loss: 0.24345\n",
      "Step [632/505]: Loss: 0.24306\n",
      "Step [633/505]: Loss: 0.24331\n",
      "Step [634/505]: Loss: 0.24294\n",
      "Step [635/505]: Loss: 0.24256\n",
      "Step [636/505]: Loss: 0.24218\n",
      "Step [637/505]: Loss: 0.24180\n",
      "Step [638/505]: Loss: 0.24142\n",
      "Step [639/505]: Loss: 0.24146\n",
      "Step [640/505]: Loss: 0.24109\n",
      "Step [641/505]: Loss: 0.24072\n",
      "Step [642/505]: Loss: 0.24034\n",
      "Step [643/505]: Loss: 0.24004\n",
      "Step [644/505]: Loss: 0.23968\n",
      "Step [645/505]: Loss: 0.23932\n",
      "Step [646/505]: Loss: 0.23895\n",
      "Step [647/505]: Loss: 0.23862\n",
      "Step [648/505]: Loss: 0.23835\n",
      "Step [649/505]: Loss: 0.23798\n",
      "Step [650/505]: Loss: 0.23765\n",
      "Step [651/505]: Loss: 0.23764\n",
      "Step [652/505]: Loss: 0.23731\n",
      "Step [653/505]: Loss: 0.23695\n",
      "Step [654/505]: Loss: 0.23831\n",
      "Step [655/505]: Loss: 0.23795\n",
      "Step [656/505]: Loss: 0.23759\n",
      "Step [657/505]: Loss: 0.23730\n",
      "Step [658/505]: Loss: 0.23888\n",
      "Step [659/505]: Loss: 0.23852\n",
      "Step [660/505]: Loss: 0.23821\n",
      "Step [661/505]: Loss: 0.23785\n",
      "Step [662/505]: Loss: 0.23941\n",
      "Step [663/505]: Loss: 0.24225\n",
      "Step [664/505]: Loss: 0.24188\n",
      "Step [665/505]: Loss: 0.24152\n",
      "Step [666/505]: Loss: 0.24117\n",
      "Step [667/505]: Loss: 0.24371\n",
      "Step [668/505]: Loss: 0.24340\n",
      "Step [669/505]: Loss: 0.24304\n",
      "Step [670/505]: Loss: 0.24363\n",
      "Step [671/505]: Loss: 0.24327\n",
      "Step [672/505]: Loss: 0.24300\n",
      "Step [673/505]: Loss: 0.24264\n",
      "Step [674/505]: Loss: 0.24231\n",
      "Step [675/505]: Loss: 0.24196\n",
      "Step [676/505]: Loss: 0.24160\n",
      "Step [677/505]: Loss: 0.24125\n",
      "Step [678/505]: Loss: 0.24089\n",
      "Step [679/505]: Loss: 0.24054\n",
      "Step [680/505]: Loss: 0.24032\n",
      "Step [681/505]: Loss: 0.24001\n",
      "Step [682/505]: Loss: 0.23966\n",
      "Step [683/505]: Loss: 0.23931\n",
      "Step [684/505]: Loss: 0.23908\n",
      "Step [685/505]: Loss: 0.23876\n",
      "Step [686/505]: Loss: 0.23922\n",
      "Step [687/505]: Loss: 0.24029\n",
      "Step [688/505]: Loss: 0.23994\n",
      "Step [689/505]: Loss: 0.23960\n",
      "Step [690/505]: Loss: 0.23925\n",
      "Step [691/505]: Loss: 0.23891\n",
      "Step [692/505]: Loss: 0.23857\n",
      "Step [693/505]: Loss: 0.23823\n",
      "Step [694/505]: Loss: 0.23788\n",
      "Step [695/505]: Loss: 0.23754\n",
      "Step [696/505]: Loss: 0.23854\n",
      "Step [697/505]: Loss: 0.23820\n",
      "Step [698/505]: Loss: 0.23786\n",
      "Step [699/505]: Loss: 0.23805\n",
      "Step [700/505]: Loss: 0.23771\n",
      "Step [701/505]: Loss: 0.23738\n",
      "Step [702/505]: Loss: 0.23709\n",
      "Step [703/505]: Loss: 0.23776\n",
      "Step [704/505]: Loss: 0.23742\n",
      "Step [705/505]: Loss: 0.23739\n",
      "Step [706/505]: Loss: 0.23712\n",
      "Step [707/505]: Loss: 0.23678\n",
      "Step [708/505]: Loss: 0.23645\n",
      "Step [709/505]: Loss: 0.23617\n",
      "Step [710/505]: Loss: 0.23596\n",
      "Step [711/505]: Loss: 0.23699\n",
      "Step [712/505]: Loss: 0.23701\n",
      "Step [713/505]: Loss: 0.23668\n",
      "Step [714/505]: Loss: 0.23635\n",
      "Step [715/505]: Loss: 0.23602\n",
      "Step [716/505]: Loss: 0.23569\n",
      "Step [717/505]: Loss: 0.23536\n",
      "Step [718/505]: Loss: 0.23534\n",
      "Step [719/505]: Loss: 0.23502\n",
      "Step [720/505]: Loss: 0.23472\n",
      "Step [721/505]: Loss: 0.23440\n",
      "Step [722/505]: Loss: 0.23426\n",
      "Step [723/505]: Loss: 0.23393\n",
      "Step [724/505]: Loss: 0.23484\n",
      "Step [725/505]: Loss: 0.23521\n",
      "Step [726/505]: Loss: 0.23561\n",
      "Step [727/505]: Loss: 0.23528\n",
      "Step [728/505]: Loss: 0.23496\n",
      "Step [729/505]: Loss: 0.23464\n",
      "Step [730/505]: Loss: 0.23432\n",
      "Step [731/505]: Loss: 0.23400\n",
      "Step [732/505]: Loss: 0.23369\n",
      "Step [733/505]: Loss: 0.23338\n",
      "Step [734/505]: Loss: 0.23318\n",
      "Step [735/505]: Loss: 0.23294\n",
      "Step [736/505]: Loss: 0.23263\n",
      "Step [737/505]: Loss: 0.23233\n",
      "Step [738/505]: Loss: 0.23217\n",
      "Step [739/505]: Loss: 0.23186\n",
      "Step [740/505]: Loss: 0.23172\n",
      "Step [741/505]: Loss: 0.23150\n",
      "Step [742/505]: Loss: 0.23119\n",
      "Step [743/505]: Loss: 0.23092\n",
      "Step [744/505]: Loss: 0.23061\n",
      "Step [745/505]: Loss: 0.23030\n",
      "Step [746/505]: Loss: 0.23000\n",
      "Step [747/505]: Loss: 0.22969\n",
      "Step [748/505]: Loss: 0.23076\n",
      "Step [749/505]: Loss: 0.23509\n",
      "Step [750/505]: Loss: 0.23683\n",
      "Step [751/505]: Loss: 0.23652\n",
      "Step [752/505]: Loss: 0.23648\n",
      "Step [753/505]: Loss: 0.23617\n",
      "Step [754/505]: Loss: 0.23586\n",
      "Step [755/505]: Loss: 0.23584\n",
      "Step [756/505]: Loss: 0.23670\n",
      "Step [757/505]: Loss: 0.23638\n",
      "Step [758/505]: Loss: 0.23696\n",
      "Step [759/505]: Loss: 0.23665\n",
      "Step [760/505]: Loss: 0.23891\n",
      "Step [761/505]: Loss: 0.23863\n",
      "Step [762/505]: Loss: 0.23832\n",
      "Step [763/505]: Loss: 0.23801\n",
      "Step [764/505]: Loss: 0.23771\n",
      "Step [765/505]: Loss: 0.23866\n",
      "Step [766/505]: Loss: 0.23888\n",
      "Step [767/505]: Loss: 0.24124\n",
      "Step [768/505]: Loss: 0.24093\n",
      "Step [769/505]: Loss: 0.24062\n",
      "Step [770/505]: Loss: 0.24031\n",
      "Step [771/505]: Loss: 0.24284\n",
      "Step [772/505]: Loss: 0.24285\n",
      "Step [773/505]: Loss: 0.24253\n",
      "Step [774/505]: Loss: 0.24222\n",
      "Step [775/505]: Loss: 0.24191\n",
      "Step [776/505]: Loss: 0.24160\n",
      "Step [777/505]: Loss: 0.24129\n",
      "Step [778/505]: Loss: 0.24244\n",
      "Step [779/505]: Loss: 0.24213\n",
      "Step [780/505]: Loss: 0.24182\n",
      "Step [781/505]: Loss: 0.24151\n",
      "Step [782/505]: Loss: 0.24121\n",
      "Step [783/505]: Loss: 0.24093\n",
      "Step [784/505]: Loss: 0.24064\n",
      "Step [785/505]: Loss: 0.24049\n",
      "Step [786/505]: Loss: 0.24019\n",
      "Step [787/505]: Loss: 0.23989\n",
      "Step [788/505]: Loss: 0.23958\n",
      "Step [789/505]: Loss: 0.23928\n",
      "Step [790/505]: Loss: 0.23898\n",
      "Step [791/505]: Loss: 0.23868\n",
      "Step [792/505]: Loss: 0.23840\n",
      "Step [793/505]: Loss: 0.23854\n",
      "Step [794/505]: Loss: 0.23858\n",
      "Step [795/505]: Loss: 0.23828\n",
      "Step [796/505]: Loss: 0.23800\n",
      "Step [797/505]: Loss: 0.23822\n",
      "Step [798/505]: Loss: 0.23830\n",
      "Step [799/505]: Loss: 0.23802\n",
      "Step [800/505]: Loss: 0.23772\n",
      "Step [801/505]: Loss: 0.23743\n",
      "Step [802/505]: Loss: 0.23714\n",
      "Step [803/505]: Loss: 0.23685\n",
      "Step [804/505]: Loss: 0.23667\n",
      "Step [805/505]: Loss: 0.23776\n",
      "Step [806/505]: Loss: 0.23747\n",
      "Step [807/505]: Loss: 0.23718\n",
      "Step [808/505]: Loss: 0.23689\n",
      "Step [809/505]: Loss: 0.23660\n",
      "Step [810/505]: Loss: 0.23632\n",
      "Step [811/505]: Loss: 0.23612\n",
      "Step [812/505]: Loss: 0.23715\n",
      "Step [813/505]: Loss: 0.23686\n",
      "Step [814/505]: Loss: 0.23657\n",
      "Step [815/505]: Loss: 0.23681\n",
      "Step [816/505]: Loss: 0.23652\n",
      "Step [817/505]: Loss: 0.23623\n",
      "Step [818/505]: Loss: 0.23594\n",
      "Step [819/505]: Loss: 0.23566\n",
      "Step [820/505]: Loss: 0.23537\n",
      "Step [821/505]: Loss: 0.23724\n",
      "Step [822/505]: Loss: 0.23695\n",
      "Step [823/505]: Loss: 0.23666\n",
      "Step [824/505]: Loss: 0.23638\n",
      "Step [825/505]: Loss: 0.23609\n",
      "Step [826/505]: Loss: 0.23580\n",
      "Step [827/505]: Loss: 0.23552\n",
      "Step [828/505]: Loss: 0.23524\n",
      "Step [829/505]: Loss: 0.23505\n",
      "Step [830/505]: Loss: 0.23586\n",
      "Step [831/505]: Loss: 0.23557\n",
      "Step [832/505]: Loss: 0.23529\n",
      "Step [833/505]: Loss: 0.23504\n",
      "Step [834/505]: Loss: 0.23512\n",
      "Step [835/505]: Loss: 0.23490\n",
      "Step [836/505]: Loss: 0.23552\n",
      "Step [837/505]: Loss: 0.23524\n",
      "Step [838/505]: Loss: 0.23499\n",
      "Step [839/505]: Loss: 0.23471\n",
      "Step [840/505]: Loss: 0.23448\n",
      "Step [841/505]: Loss: 0.23426\n",
      "Step [842/505]: Loss: 0.23400\n",
      "Step [843/505]: Loss: 0.23401\n",
      "Step [844/505]: Loss: 0.23529\n",
      "Step [845/505]: Loss: 0.23513\n",
      "Step [846/505]: Loss: 0.23486\n",
      "Step [847/505]: Loss: 0.23601\n",
      "Step [848/505]: Loss: 0.23575\n",
      "Step [849/505]: Loss: 0.23547\n",
      "Step [850/505]: Loss: 0.23520\n",
      "Step [851/505]: Loss: 0.23496\n",
      "Step [852/505]: Loss: 0.23469\n",
      "Step [853/505]: Loss: 0.23442\n",
      "Step [854/505]: Loss: 0.23416\n",
      "Step [855/505]: Loss: 0.23388\n",
      "Step [856/505]: Loss: 0.23369\n",
      "Step [857/505]: Loss: 0.23342\n",
      "Step [858/505]: Loss: 0.23315\n",
      "Step [859/505]: Loss: 0.23288\n",
      "Step [860/505]: Loss: 0.23261\n",
      "Step [861/505]: Loss: 0.23236\n",
      "Step [862/505]: Loss: 0.23209\n",
      "Step [863/505]: Loss: 0.23213\n",
      "Step [864/505]: Loss: 0.23214\n",
      "Step [865/505]: Loss: 0.23302\n",
      "Step [866/505]: Loss: 0.23458\n",
      "Step [867/505]: Loss: 0.23468\n",
      "Step [868/505]: Loss: 0.23441\n",
      "Step [869/505]: Loss: 0.23469\n",
      "Step [870/505]: Loss: 0.23452\n",
      "Step [871/505]: Loss: 0.23425\n",
      "Step [872/505]: Loss: 0.23631\n",
      "Step [873/505]: Loss: 0.23684\n",
      "Step [874/505]: Loss: 0.23685\n",
      "Step [875/505]: Loss: 0.23658\n",
      "Step [876/505]: Loss: 0.23770\n",
      "Step [877/505]: Loss: 0.23981\n",
      "Step [878/505]: Loss: 0.23954\n",
      "Step [879/505]: Loss: 0.23928\n",
      "Step [880/505]: Loss: 0.23902\n",
      "Step [881/505]: Loss: 0.23875\n",
      "Step [882/505]: Loss: 0.23848\n",
      "Step [883/505]: Loss: 0.23847\n",
      "Step [884/505]: Loss: 0.23821\n",
      "Step [885/505]: Loss: 0.23794\n",
      "Step [886/505]: Loss: 0.23767\n",
      "Step [887/505]: Loss: 0.23821\n",
      "Step [888/505]: Loss: 0.23811\n",
      "Step [889/505]: Loss: 0.23784\n",
      "Step [890/505]: Loss: 0.23758\n",
      "Step [891/505]: Loss: 0.23731\n",
      "Step [892/505]: Loss: 0.23705\n",
      "Step [893/505]: Loss: 0.23678\n",
      "Step [894/505]: Loss: 0.23730\n",
      "Step [895/505]: Loss: 0.23705\n",
      "Step [896/505]: Loss: 0.23678\n",
      "Step [897/505]: Loss: 0.23795\n",
      "Step [898/505]: Loss: 0.23790\n",
      "Step [899/505]: Loss: 0.23767\n",
      "Step [900/505]: Loss: 0.23742\n",
      "Step [901/505]: Loss: 0.23720\n",
      "Step [902/505]: Loss: 0.23694\n",
      "Step [903/505]: Loss: 0.23668\n",
      "Step [904/505]: Loss: 0.23642\n",
      "Step [905/505]: Loss: 0.23616\n",
      "Step [906/505]: Loss: 0.23621\n",
      "Step [907/505]: Loss: 0.23595\n",
      "Step [908/505]: Loss: 0.23673\n",
      "Step [909/505]: Loss: 0.23843\n",
      "Step [910/505]: Loss: 0.23817\n",
      "Step [911/505]: Loss: 0.23947\n",
      "Step [912/505]: Loss: 0.23921\n",
      "Step [913/505]: Loss: 0.23895\n",
      "Step [914/505]: Loss: 0.24093\n",
      "Step [915/505]: Loss: 0.24206\n",
      "Step [916/505]: Loss: 0.24179\n",
      "Step [917/505]: Loss: 0.24153\n",
      "Step [918/505]: Loss: 0.24332\n",
      "Step [919/505]: Loss: 0.24306\n",
      "Step [920/505]: Loss: 0.24298\n",
      "Step [921/505]: Loss: 0.24272\n",
      "Step [922/505]: Loss: 0.24246\n",
      "Step [923/505]: Loss: 0.24220\n",
      "Step [924/505]: Loss: 0.24194\n",
      "Step [925/505]: Loss: 0.24168\n",
      "Step [926/505]: Loss: 0.24143\n",
      "Step [927/505]: Loss: 0.24117\n",
      "Step [928/505]: Loss: 0.24092\n",
      "Step [929/505]: Loss: 0.24066\n",
      "Step [930/505]: Loss: 0.24040\n",
      "Step [931/505]: Loss: 0.24014\n",
      "Step [932/505]: Loss: 0.23989\n",
      "Step [933/505]: Loss: 0.23971\n",
      "Step [934/505]: Loss: 0.23958\n",
      "Step [935/505]: Loss: 0.23933\n",
      "Step [936/505]: Loss: 0.23931\n",
      "Step [937/505]: Loss: 0.23981\n",
      "Step [938/505]: Loss: 0.23956\n",
      "Step [939/505]: Loss: 0.23930\n",
      "Step [940/505]: Loss: 0.23905\n",
      "Step [941/505]: Loss: 0.23879\n",
      "Step [942/505]: Loss: 0.23971\n",
      "Step [943/505]: Loss: 0.24149\n",
      "Step [944/505]: Loss: 0.24126\n",
      "Step [945/505]: Loss: 0.24101\n",
      "Step [946/505]: Loss: 0.24076\n",
      "Step [947/505]: Loss: 0.24061\n",
      "Step [948/505]: Loss: 0.24092\n",
      "Step [949/505]: Loss: 0.24067\n",
      "Step [950/505]: Loss: 0.24042\n",
      "Step [951/505]: Loss: 0.24030\n",
      "Step [952/505]: Loss: 0.24004\n",
      "Step [953/505]: Loss: 0.24005\n",
      "Step [954/505]: Loss: 0.24016\n",
      "Step [955/505]: Loss: 0.23995\n",
      "Step [956/505]: Loss: 0.23970\n",
      "Step [957/505]: Loss: 0.23945\n",
      "Step [958/505]: Loss: 0.23920\n",
      "Step [959/505]: Loss: 0.23896\n",
      "Step [960/505]: Loss: 0.23871\n",
      "Step [961/505]: Loss: 0.23846\n",
      "Step [962/505]: Loss: 0.23822\n",
      "Step [963/505]: Loss: 0.23797\n",
      "Step [964/505]: Loss: 0.23772\n",
      "Step [965/505]: Loss: 0.23748\n",
      "Step [966/505]: Loss: 0.23723\n",
      "Step [967/505]: Loss: 0.23701\n",
      "Step [968/505]: Loss: 0.23678\n",
      "Step [969/505]: Loss: 0.23734\n",
      "Step [970/505]: Loss: 0.23824\n",
      "Step [971/505]: Loss: 0.23936\n",
      "Step [972/505]: Loss: 0.23912\n",
      "Step [973/505]: Loss: 0.23896\n",
      "Step [974/505]: Loss: 0.24025\n",
      "Step [975/505]: Loss: 0.24001\n",
      "Step [976/505]: Loss: 0.23976\n",
      "Step [977/505]: Loss: 0.23958\n",
      "Step [978/505]: Loss: 0.23946\n",
      "Step [979/505]: Loss: 0.23921\n",
      "Step [980/505]: Loss: 0.23897\n",
      "Step [981/505]: Loss: 0.23873\n",
      "Step [982/505]: Loss: 0.23867\n",
      "Step [983/505]: Loss: 0.23842\n",
      "Step [984/505]: Loss: 0.23818\n",
      "Step [985/505]: Loss: 0.23798\n",
      "Step [986/505]: Loss: 0.23774\n",
      "Step [987/505]: Loss: 0.23750\n",
      "Step [988/505]: Loss: 0.23728\n",
      "Step [989/505]: Loss: 0.23704\n",
      "Step [990/505]: Loss: 0.23681\n",
      "Step [991/505]: Loss: 0.23791\n",
      "Step [992/505]: Loss: 0.23768\n",
      "Step [993/505]: Loss: 0.23930\n",
      "Step [994/505]: Loss: 0.23952\n",
      "Step [995/505]: Loss: 0.23930\n",
      "Step [996/505]: Loss: 0.23915\n",
      "Step [997/505]: Loss: 0.23891\n",
      "Step [998/505]: Loss: 0.23868\n",
      "Step [999/505]: Loss: 0.23844\n",
      "Step [1000/505]: Loss: 0.23820\n",
      "Step [1001/505]: Loss: 0.23855\n",
      "Step [1002/505]: Loss: 0.23831\n",
      "Step [1003/505]: Loss: 0.23808\n",
      "Step [1004/505]: Loss: 0.23784\n",
      "Step [1005/505]: Loss: 0.23761\n",
      "Step [1006/505]: Loss: 0.23738\n",
      "Step [1007/505]: Loss: 0.23811\n",
      "Step [1008/505]: Loss: 0.23792\n",
      "Step [1009/505]: Loss: 0.23929\n",
      "Step [1010/505]: Loss: 0.23923\n",
      "Step [1011/505]: Loss: 0.23939\n",
      "Step [1012/505]: Loss: 0.23987\n",
      "Step [1013/505]: Loss: 0.23964\n",
      "Step [1014/505]: Loss: 0.23941\n",
      "Step [1015/505]: Loss: 0.23932\n",
      "Step [1016/505]: Loss: 0.23909\n",
      "Step [1017/505]: Loss: 0.23895\n",
      "Step [1018/505]: Loss: 0.23871\n",
      "Step [1019/505]: Loss: 0.23848\n",
      "Step [1020/505]: Loss: 0.23825\n",
      "Step [1021/505]: Loss: 0.23801\n",
      "Step [1022/505]: Loss: 0.23786\n",
      "Step [1023/505]: Loss: 0.23763\n",
      "Step [1024/505]: Loss: 0.23740\n",
      "Step [1025/505]: Loss: 0.23790\n",
      "Step [1026/505]: Loss: 0.23856\n",
      "Step [1027/505]: Loss: 0.23836\n",
      "Step [1028/505]: Loss: 0.23815\n",
      "Step [1029/505]: Loss: 0.23804\n",
      "Step [1030/505]: Loss: 0.23781\n",
      "Step [1031/505]: Loss: 0.23758\n",
      "Step [1032/505]: Loss: 0.23735\n",
      "Step [1033/505]: Loss: 0.23712\n",
      "Step [1034/505]: Loss: 0.23689\n",
      "Step [1035/505]: Loss: 0.23667\n",
      "Step [1036/505]: Loss: 0.23645\n",
      "Step [1037/505]: Loss: 0.23622\n",
      "Step [1038/505]: Loss: 0.23600\n",
      "Step [1039/505]: Loss: 0.23578\n",
      "Step [1040/505]: Loss: 0.23555\n",
      "Step [1041/505]: Loss: 0.23533\n",
      "Step [1042/505]: Loss: 0.23510\n",
      "Step [1043/505]: Loss: 0.23488\n",
      "Step [1044/505]: Loss: 0.23467\n",
      "Step [1045/505]: Loss: 0.23447\n",
      "Step [1046/505]: Loss: 0.23426\n",
      "Step [1047/505]: Loss: 0.23426\n",
      "Step [1048/505]: Loss: 0.23598\n",
      "Step [1049/505]: Loss: 0.23576\n",
      "Step [1050/505]: Loss: 0.23554\n",
      "Step [1051/505]: Loss: 0.23531\n",
      "Step [1052/505]: Loss: 0.23509\n",
      "Step [1053/505]: Loss: 0.23487\n",
      "Step [1054/505]: Loss: 0.23465\n",
      "Step [1055/505]: Loss: 0.23525\n",
      "Step [1056/505]: Loss: 0.23504\n",
      "Step [1057/505]: Loss: 0.23488\n",
      "Step [1058/505]: Loss: 0.23466\n",
      "Step [1059/505]: Loss: 0.23446\n",
      "Step [1060/505]: Loss: 0.23424\n",
      "Step [1061/505]: Loss: 0.23402\n",
      "Step [1062/505]: Loss: 0.23381\n",
      "Step [1063/505]: Loss: 0.23360\n",
      "Step [1064/505]: Loss: 0.23338\n",
      "Step [1065/505]: Loss: 0.23512\n",
      "Step [1066/505]: Loss: 0.23551\n",
      "Step [1067/505]: Loss: 0.23529\n",
      "Step [1068/505]: Loss: 0.23507\n",
      "Step [1069/505]: Loss: 0.23497\n",
      "Step [1070/505]: Loss: 0.23475\n",
      "Step [1071/505]: Loss: 0.23454\n",
      "Step [1072/505]: Loss: 0.23506\n",
      "Step [1073/505]: Loss: 0.23485\n",
      "Step [1074/505]: Loss: 0.23464\n",
      "Step [1075/505]: Loss: 0.23610\n",
      "Step [1076/505]: Loss: 0.23588\n",
      "Step [1077/505]: Loss: 0.23566\n",
      "Step [1078/505]: Loss: 0.23688\n",
      "Step [1079/505]: Loss: 0.23666\n",
      "Step [1080/505]: Loss: 0.23774\n",
      "Step [1081/505]: Loss: 0.23753\n",
      "Step [1082/505]: Loss: 0.23731\n",
      "Step [1083/505]: Loss: 0.23743\n",
      "Step [1084/505]: Loss: 0.23721\n",
      "Step [1085/505]: Loss: 0.23700\n",
      "Step [1086/505]: Loss: 0.23678\n",
      "Step [1087/505]: Loss: 0.23791\n",
      "Step [1088/505]: Loss: 0.23780\n",
      "Step [1089/505]: Loss: 0.23766\n",
      "Step [1090/505]: Loss: 0.23746\n",
      "Step [1091/505]: Loss: 0.23789\n",
      "Step [1092/505]: Loss: 0.23824\n",
      "Step [1093/505]: Loss: 0.23802\n",
      "Step [1094/505]: Loss: 0.23780\n",
      "Step [1095/505]: Loss: 0.23759\n",
      "Step [1096/505]: Loss: 0.23737\n",
      "Step [1097/505]: Loss: 0.23716\n",
      "Step [1098/505]: Loss: 0.23695\n",
      "Step [1099/505]: Loss: 0.23763\n",
      "Step [1100/505]: Loss: 0.23742\n",
      "Step [1101/505]: Loss: 0.23720\n",
      "Step [1102/505]: Loss: 0.23747\n",
      "Step [1103/505]: Loss: 0.23732\n",
      "Step [1104/505]: Loss: 0.23724\n",
      "Step [1105/505]: Loss: 0.23702\n",
      "Step [1106/505]: Loss: 0.23751\n",
      "Step [1107/505]: Loss: 0.23730\n",
      "Step [1108/505]: Loss: 0.23709\n",
      "Step [1109/505]: Loss: 0.23688\n",
      "Step [1110/505]: Loss: 0.23746\n",
      "Step [1111/505]: Loss: 0.23724\n",
      "Step [1112/505]: Loss: 0.23757\n",
      "Step [1113/505]: Loss: 0.23740\n",
      "Step [1114/505]: Loss: 0.23723\n",
      "Step [1115/505]: Loss: 0.23722\n",
      "Step [1116/505]: Loss: 0.23852\n",
      "Step [1117/505]: Loss: 0.23911\n",
      "Step [1118/505]: Loss: 0.23954\n",
      "Step [1119/505]: Loss: 0.24074\n",
      "Step [1120/505]: Loss: 0.24053\n",
      "Step [1121/505]: Loss: 0.24032\n",
      "Step [1122/505]: Loss: 0.24011\n",
      "Step [1123/505]: Loss: 0.23989\n",
      "Step [1124/505]: Loss: 0.23968\n",
      "Step [1125/505]: Loss: 0.23947\n",
      "Step [1126/505]: Loss: 0.23977\n",
      "Step [1127/505]: Loss: 0.24099\n",
      "Step [1128/505]: Loss: 0.24077\n",
      "Step [1129/505]: Loss: 0.24056\n",
      "Step [1130/505]: Loss: 0.24035\n",
      "Step [1131/505]: Loss: 0.24015\n",
      "Step [1132/505]: Loss: 0.24146\n",
      "Step [1133/505]: Loss: 0.24125\n",
      "Step [1134/505]: Loss: 0.24104\n",
      "Step [1135/505]: Loss: 0.24088\n",
      "Step [1136/505]: Loss: 0.24066\n",
      "Step [1137/505]: Loss: 0.24045\n",
      "Step [1138/505]: Loss: 0.24079\n",
      "Step [1139/505]: Loss: 0.24058\n",
      "Step [1140/505]: Loss: 0.24061\n",
      "Step [1141/505]: Loss: 0.24052\n",
      "Step [1142/505]: Loss: 0.24031\n",
      "Step [1143/505]: Loss: 0.24010\n",
      "Step [1144/505]: Loss: 0.23989\n",
      "Step [1145/505]: Loss: 0.23968\n",
      "Step [1146/505]: Loss: 0.23947\n",
      "Step [1147/505]: Loss: 0.23927\n",
      "Step [1148/505]: Loss: 0.23907\n",
      "Step [1149/505]: Loss: 0.23886\n",
      "Step [1150/505]: Loss: 0.23865\n",
      "Step [1151/505]: Loss: 0.23898\n",
      "Step [1152/505]: Loss: 0.23878\n",
      "Step [1153/505]: Loss: 0.23857\n",
      "Step [1154/505]: Loss: 0.23837\n",
      "Step [1155/505]: Loss: 0.23873\n",
      "Step [1156/505]: Loss: 0.23852\n",
      "Step [1157/505]: Loss: 0.23832\n",
      "Step [1158/505]: Loss: 0.23811\n",
      "Step [1159/505]: Loss: 0.23791\n",
      "Step [1160/505]: Loss: 0.23775\n",
      "Step [1161/505]: Loss: 0.23755\n",
      "Step [1162/505]: Loss: 0.23735\n",
      "Step [1163/505]: Loss: 0.23715\n",
      "Step [1164/505]: Loss: 0.23695\n",
      "Step [1165/505]: Loss: 0.23675\n",
      "Step [1166/505]: Loss: 0.23664\n",
      "Step [1167/505]: Loss: 0.23645\n",
      "Step [1168/505]: Loss: 0.23667\n",
      "Step [1169/505]: Loss: 0.23647\n",
      "Step [1170/505]: Loss: 0.23629\n",
      "Step [1171/505]: Loss: 0.23613\n",
      "Step [1172/505]: Loss: 0.23593\n",
      "Step [1173/505]: Loss: 0.23748\n",
      "Step [1174/505]: Loss: 0.23816\n",
      "Step [1175/505]: Loss: 0.23796\n",
      "Step [1176/505]: Loss: 0.23776\n",
      "Step [1177/505]: Loss: 0.23756\n",
      "Step [1178/505]: Loss: 0.23737\n",
      "Step [1179/505]: Loss: 0.23718\n",
      "Step [1180/505]: Loss: 0.23702\n",
      "Step [1181/505]: Loss: 0.23684\n",
      "Step [1182/505]: Loss: 0.23664\n",
      "Step [1183/505]: Loss: 0.23644\n",
      "Step [1184/505]: Loss: 0.23624\n",
      "Step [1185/505]: Loss: 0.23608\n",
      "Step [1186/505]: Loss: 0.23590\n",
      "Step [1187/505]: Loss: 0.23716\n",
      "Step [1188/505]: Loss: 0.23696\n",
      "Step [1189/505]: Loss: 0.23677\n",
      "Step [1190/505]: Loss: 0.23657\n",
      "Step [1191/505]: Loss: 0.23723\n",
      "Step [1192/505]: Loss: 0.23726\n",
      "Step [1193/505]: Loss: 0.23706\n",
      "Step [1194/505]: Loss: 0.23786\n",
      "Step [1195/505]: Loss: 0.23805\n",
      "Step [1196/505]: Loss: 0.23822\n",
      "Step [1197/505]: Loss: 0.23853\n",
      "Step [1198/505]: Loss: 0.23970\n",
      "Step [1199/505]: Loss: 0.23951\n",
      "Step [1200/505]: Loss: 0.23931\n",
      "Step [1201/505]: Loss: 0.24072\n",
      "Step [1202/505]: Loss: 0.24068\n",
      "Step [1203/505]: Loss: 0.24048\n",
      "Step [1204/505]: Loss: 0.24219\n",
      "Step [1205/505]: Loss: 0.24201\n",
      "Step [1206/505]: Loss: 0.24181\n",
      "Step [1207/505]: Loss: 0.24272\n",
      "Step [1208/505]: Loss: 0.24323\n",
      "Step [1209/505]: Loss: 0.24311\n",
      "Step [1210/505]: Loss: 0.24294\n",
      "Step [1211/505]: Loss: 0.24379\n",
      "Step [1212/505]: Loss: 0.24359\n",
      "Step [1213/505]: Loss: 0.24512\n",
      "Step [1214/505]: Loss: 0.24492\n",
      "Step [1215/505]: Loss: 0.24472\n",
      "Step [1216/505]: Loss: 0.24483\n",
      "Step [1217/505]: Loss: 0.24463\n",
      "Step [1218/505]: Loss: 0.24444\n",
      "Step [1219/505]: Loss: 0.24424\n",
      "Step [1220/505]: Loss: 0.24404\n",
      "Step [1221/505]: Loss: 0.24385\n",
      "Step [1222/505]: Loss: 0.24367\n",
      "Step [1223/505]: Loss: 0.24347\n",
      "Step [1224/505]: Loss: 0.24333\n",
      "Step [1225/505]: Loss: 0.24353\n",
      "Step [1226/505]: Loss: 0.24464\n",
      "Step [1227/505]: Loss: 0.24483\n",
      "Step [1228/505]: Loss: 0.24633\n",
      "Step [1229/505]: Loss: 0.24613\n",
      "Step [1230/505]: Loss: 0.24744\n",
      "Step [1231/505]: Loss: 0.24746\n",
      "Step [1232/505]: Loss: 0.24875\n",
      "Step [1233/505]: Loss: 0.24863\n",
      "Step [1234/505]: Loss: 0.24844\n",
      "Step [1235/505]: Loss: 0.24825\n",
      "Step [1236/505]: Loss: 0.24808\n",
      "Step [1237/505]: Loss: 0.24791\n",
      "Step [1238/505]: Loss: 0.24774\n",
      "Step [1239/505]: Loss: 0.24757\n",
      "Step [1240/505]: Loss: 0.24768\n",
      "Step [1241/505]: Loss: 0.24748\n",
      "Step [1242/505]: Loss: 0.24749\n",
      "Step [1243/505]: Loss: 0.24730\n",
      "Step [1244/505]: Loss: 0.24710\n",
      "Step [1245/505]: Loss: 0.24768\n",
      "Step [1246/505]: Loss: 0.24751\n",
      "Step [1247/505]: Loss: 0.24793\n",
      "Step [1248/505]: Loss: 0.24775\n",
      "Step [1249/505]: Loss: 0.24763\n",
      "Step [1250/505]: Loss: 0.24745\n",
      "Step [1251/505]: Loss: 0.24725\n",
      "Step [1252/505]: Loss: 0.24705\n",
      "Step [1253/505]: Loss: 0.24685\n",
      "Step [1254/505]: Loss: 0.24666\n",
      "Step [1255/505]: Loss: 0.24646\n",
      "Step [1256/505]: Loss: 0.24627\n",
      "Step [1257/505]: Loss: 0.24612\n",
      "Step [1258/505]: Loss: 0.24593\n",
      "Step [1259/505]: Loss: 0.24573\n",
      "Step [1260/505]: Loss: 0.24554\n",
      "Step [1261/505]: Loss: 0.24554\n",
      "Step [1262/505]: Loss: 0.24535\n",
      "Step [1263/505]: Loss: 0.24515\n",
      "Step [1264/505]: Loss: 0.24498\n",
      "Step [1265/505]: Loss: 0.24540\n",
      "Step [1266/505]: Loss: 0.24521\n",
      "Step [1267/505]: Loss: 0.24502\n",
      "Step [1268/505]: Loss: 0.24482\n",
      "Step [1269/505]: Loss: 0.24463\n",
      "Step [1270/505]: Loss: 0.24445\n",
      "Step [1271/505]: Loss: 0.24426\n",
      "Step [1272/505]: Loss: 0.24406\n",
      "Step [1273/505]: Loss: 0.24387\n",
      "Step [1274/505]: Loss: 0.24369\n",
      "Step [1275/505]: Loss: 0.24364\n",
      "Step [1276/505]: Loss: 0.24359\n",
      "Step [1277/505]: Loss: 0.24340\n",
      "Step [1278/505]: Loss: 0.24322\n",
      "Step [1279/505]: Loss: 0.24313\n",
      "Step [1280/505]: Loss: 0.24295\n",
      "Step [1281/505]: Loss: 0.24279\n",
      "Step [1282/505]: Loss: 0.24260\n",
      "Step [1283/505]: Loss: 0.24241\n",
      "Step [1284/505]: Loss: 0.24223\n",
      "Step [1285/505]: Loss: 0.24204\n",
      "Step [1286/505]: Loss: 0.24185\n",
      "Step [1287/505]: Loss: 0.24166\n",
      "Step [1288/505]: Loss: 0.24245\n",
      "Step [1289/505]: Loss: 0.24226\n",
      "Step [1290/505]: Loss: 0.24207\n",
      "Step [1291/505]: Loss: 0.24222\n",
      "Step [1292/505]: Loss: 0.24203\n",
      "Step [1293/505]: Loss: 0.24224\n",
      "Step [1294/505]: Loss: 0.24211\n",
      "Step [1295/505]: Loss: 0.24192\n",
      "Step [1296/505]: Loss: 0.24178\n",
      "Step [1297/505]: Loss: 0.24160\n",
      "Step [1298/505]: Loss: 0.24141\n",
      "Step [1299/505]: Loss: 0.24123\n",
      "Step [1300/505]: Loss: 0.24105\n",
      "Step [1301/505]: Loss: 0.24086\n",
      "Step [1302/505]: Loss: 0.24069\n",
      "Step [1303/505]: Loss: 0.24051\n",
      "Step [1304/505]: Loss: 0.24033\n",
      "Step [1305/505]: Loss: 0.24016\n",
      "Step [1306/505]: Loss: 0.23998\n",
      "Step [1307/505]: Loss: 0.24136\n",
      "Step [1308/505]: Loss: 0.24118\n",
      "Step [1309/505]: Loss: 0.24102\n",
      "Step [1310/505]: Loss: 0.24084\n",
      "Step [1311/505]: Loss: 0.24069\n",
      "Step [1312/505]: Loss: 0.24050\n",
      "Step [1313/505]: Loss: 0.24036\n",
      "Step [1314/505]: Loss: 0.24023\n",
      "Step [1315/505]: Loss: 0.24005\n",
      "Step [1316/505]: Loss: 0.23994\n",
      "Step [1317/505]: Loss: 0.23976\n",
      "Step [1318/505]: Loss: 0.23958\n",
      "Step [1319/505]: Loss: 0.23988\n",
      "Step [1320/505]: Loss: 0.23972\n",
      "Step [1321/505]: Loss: 0.23959\n",
      "Step [1322/505]: Loss: 0.23947\n",
      "Step [1323/505]: Loss: 0.24040\n",
      "Step [1324/505]: Loss: 0.24055\n",
      "Step [1325/505]: Loss: 0.24037\n",
      "Step [1326/505]: Loss: 0.24020\n",
      "Step [1327/505]: Loss: 0.24002\n",
      "Step [1328/505]: Loss: 0.23984\n",
      "Step [1329/505]: Loss: 0.23966\n",
      "Step [1330/505]: Loss: 0.23949\n",
      "Step [1331/505]: Loss: 0.24018\n",
      "Step [1332/505]: Loss: 0.24000\n",
      "Step [1333/505]: Loss: 0.23982\n",
      "Step [1334/505]: Loss: 0.23996\n",
      "Step [1335/505]: Loss: 0.24120\n",
      "Step [1336/505]: Loss: 0.24148\n",
      "Step [1337/505]: Loss: 0.24132\n",
      "Step [1338/505]: Loss: 0.24114\n",
      "Step [1339/505]: Loss: 0.24120\n",
      "Step [1340/505]: Loss: 0.24102\n",
      "Step [1341/505]: Loss: 0.24084\n",
      "Step [1342/505]: Loss: 0.24068\n",
      "Step [1343/505]: Loss: 0.24051\n",
      "Step [1344/505]: Loss: 0.24033\n",
      "Step [1345/505]: Loss: 0.24015\n",
      "Step [1346/505]: Loss: 0.24002\n",
      "Step [1347/505]: Loss: 0.23984\n",
      "Step [1348/505]: Loss: 0.24073\n",
      "Step [1349/505]: Loss: 0.24064\n",
      "Step [1350/505]: Loss: 0.24054\n",
      "Step [1351/505]: Loss: 0.24037\n",
      "Step [1352/505]: Loss: 0.24019\n",
      "Step [1353/505]: Loss: 0.24001\n",
      "Step [1354/505]: Loss: 0.23984\n",
      "Step [1355/505]: Loss: 0.23966\n",
      "Step [1356/505]: Loss: 0.23950\n",
      "Step [1357/505]: Loss: 0.24055\n",
      "Step [1358/505]: Loss: 0.24046\n",
      "Step [1359/505]: Loss: 0.24053\n",
      "Step [1360/505]: Loss: 0.24167\n",
      "Step [1361/505]: Loss: 0.24197\n",
      "Step [1362/505]: Loss: 0.24181\n",
      "Step [1363/505]: Loss: 0.24165\n",
      "Step [1364/505]: Loss: 0.24147\n",
      "Step [1365/505]: Loss: 0.24130\n",
      "Step [1366/505]: Loss: 0.24148\n",
      "Step [1367/505]: Loss: 0.24138\n",
      "Step [1368/505]: Loss: 0.24121\n",
      "Step [1369/505]: Loss: 0.24103\n",
      "Step [1370/505]: Loss: 0.24085\n",
      "Step [1371/505]: Loss: 0.24068\n",
      "Step [1372/505]: Loss: 0.24054\n",
      "Step [1373/505]: Loss: 0.24037\n",
      "Step [1374/505]: Loss: 0.24021\n",
      "Step [1375/505]: Loss: 0.24008\n",
      "Step [1376/505]: Loss: 0.23991\n",
      "Step [1377/505]: Loss: 0.23975\n",
      "Step [1378/505]: Loss: 0.23959\n",
      "Step [1379/505]: Loss: 0.23947\n",
      "Step [1380/505]: Loss: 0.23930\n",
      "Step [1381/505]: Loss: 0.23912\n",
      "Step [1382/505]: Loss: 0.23895\n",
      "Step [1383/505]: Loss: 0.23878\n",
      "Step [1384/505]: Loss: 0.23861\n",
      "Step [1385/505]: Loss: 0.23849\n",
      "Step [1386/505]: Loss: 0.23833\n",
      "Step [1387/505]: Loss: 0.23816\n",
      "Step [1388/505]: Loss: 0.23799\n",
      "Step [1389/505]: Loss: 0.23783\n",
      "Step [1390/505]: Loss: 0.23769\n",
      "Step [1391/505]: Loss: 0.23770\n",
      "Step [1392/505]: Loss: 0.23753\n",
      "Step [1393/505]: Loss: 0.23736\n",
      "Step [1394/505]: Loss: 0.23719\n",
      "Step [1395/505]: Loss: 0.23702\n",
      "Step [1396/505]: Loss: 0.23727\n",
      "Step [1397/505]: Loss: 0.23711\n",
      "Step [1398/505]: Loss: 0.23695\n",
      "Step [1399/505]: Loss: 0.23678\n",
      "Step [1400/505]: Loss: 0.23661\n",
      "Step [1401/505]: Loss: 0.23644\n",
      "Step [1402/505]: Loss: 0.23630\n",
      "Step [1403/505]: Loss: 0.23725\n",
      "Step [1404/505]: Loss: 0.23897\n",
      "Step [1405/505]: Loss: 0.23880\n",
      "Step [1406/505]: Loss: 0.23863\n",
      "Step [1407/505]: Loss: 0.23846\n",
      "Step [1408/505]: Loss: 0.23830\n",
      "Step [1409/505]: Loss: 0.23867\n",
      "Step [1410/505]: Loss: 0.23884\n",
      "Step [1411/505]: Loss: 0.23913\n",
      "Step [1412/505]: Loss: 0.23897\n",
      "Step [1413/505]: Loss: 0.23880\n",
      "Step [1414/505]: Loss: 0.23863\n",
      "Step [1415/505]: Loss: 0.23894\n",
      "Step [1416/505]: Loss: 0.24021\n",
      "Step [1417/505]: Loss: 0.24004\n",
      "Step [1418/505]: Loss: 0.23989\n",
      "Step [1419/505]: Loss: 0.24006\n",
      "Step [1420/505]: Loss: 0.23989\n",
      "Step [1421/505]: Loss: 0.24078\n",
      "Step [1422/505]: Loss: 0.24082\n",
      "Step [1423/505]: Loss: 0.24077\n",
      "Step [1424/505]: Loss: 0.24060\n",
      "Step [1425/505]: Loss: 0.24046\n",
      "Step [1426/505]: Loss: 0.24029\n",
      "Step [1427/505]: Loss: 0.24012\n",
      "Step [1428/505]: Loss: 0.23995\n",
      "Step [1429/505]: Loss: 0.23978\n",
      "Step [1430/505]: Loss: 0.23963\n",
      "Step [1431/505]: Loss: 0.23948\n",
      "Step [1432/505]: Loss: 0.23932\n",
      "Step [1433/505]: Loss: 0.23915\n",
      "Step [1434/505]: Loss: 0.23900\n",
      "Step [1435/505]: Loss: 0.23883\n",
      "Step [1436/505]: Loss: 0.23867\n",
      "Step [1437/505]: Loss: 0.23855\n",
      "Step [1438/505]: Loss: 0.23840\n",
      "Step [1439/505]: Loss: 0.23824\n",
      "Step [1440/505]: Loss: 0.23835\n",
      "Step [1441/505]: Loss: 0.23851\n",
      "Step [1442/505]: Loss: 0.23835\n",
      "Step [1443/505]: Loss: 0.23836\n",
      "Step [1444/505]: Loss: 0.23820\n",
      "Step [1445/505]: Loss: 0.23804\n",
      "Step [1446/505]: Loss: 0.23831\n",
      "Step [1447/505]: Loss: 0.23815\n",
      "Step [1448/505]: Loss: 0.23845\n",
      "Step [1449/505]: Loss: 0.23916\n",
      "Step [1450/505]: Loss: 0.23900\n",
      "Step [1451/505]: Loss: 0.23905\n",
      "Step [1452/505]: Loss: 0.23889\n",
      "Step [1453/505]: Loss: 0.23993\n",
      "Step [1454/505]: Loss: 0.24017\n",
      "Step [1455/505]: Loss: 0.24001\n",
      "Step [1456/505]: Loss: 0.23985\n",
      "Step [1457/505]: Loss: 0.24000\n",
      "Step [1458/505]: Loss: 0.23984\n",
      "Step [1459/505]: Loss: 0.23967\n",
      "Step [1460/505]: Loss: 0.23986\n",
      "Step [1461/505]: Loss: 0.23993\n",
      "Step [1462/505]: Loss: 0.23986\n",
      "Step [1463/505]: Loss: 0.24005\n",
      "Step [1464/505]: Loss: 0.24100\n",
      "Step [1465/505]: Loss: 0.24090\n",
      "Step [1466/505]: Loss: 0.24073\n",
      "Step [1467/505]: Loss: 0.24057\n",
      "Step [1468/505]: Loss: 0.24067\n",
      "Step [1469/505]: Loss: 0.24051\n",
      "Step [1470/505]: Loss: 0.24179\n",
      "Step [1471/505]: Loss: 0.24163\n",
      "Step [1472/505]: Loss: 0.24147\n",
      "Step [1473/505]: Loss: 0.24131\n",
      "Step [1474/505]: Loss: 0.24115\n",
      "Step [1475/505]: Loss: 0.24161\n",
      "Step [1476/505]: Loss: 0.24145\n",
      "Step [1477/505]: Loss: 0.24150\n",
      "Step [1478/505]: Loss: 0.24134\n",
      "Step [1479/505]: Loss: 0.24117\n",
      "Step [1480/505]: Loss: 0.24101\n",
      "Step [1481/505]: Loss: 0.24085\n",
      "Step [1482/505]: Loss: 0.24083\n",
      "Step [1483/505]: Loss: 0.24067\n",
      "Step [1484/505]: Loss: 0.24061\n",
      "Step [1485/505]: Loss: 0.24045\n",
      "Step [1486/505]: Loss: 0.24029\n",
      "Step [1487/505]: Loss: 0.24020\n",
      "Step [1488/505]: Loss: 0.24042\n",
      "Step [1489/505]: Loss: 0.24026\n",
      "Step [1490/505]: Loss: 0.24010\n",
      "Step [1491/505]: Loss: 0.23994\n",
      "Step [1492/505]: Loss: 0.23978\n",
      "Step [1493/505]: Loss: 0.23962\n",
      "Step [1494/505]: Loss: 0.23946\n",
      "Step [1495/505]: Loss: 0.23971\n",
      "Step [1496/505]: Loss: 0.23955\n",
      "Step [1497/505]: Loss: 0.23939\n",
      "Step [1498/505]: Loss: 0.23946\n",
      "Step [1499/505]: Loss: 0.23956\n",
      "Step [1500/505]: Loss: 0.23948\n",
      "Step [1501/505]: Loss: 0.23932\n",
      "Step [1502/505]: Loss: 0.23942\n",
      "Step [1503/505]: Loss: 0.23926\n",
      "Step [1504/505]: Loss: 0.23912\n",
      "Step [1505/505]: Loss: 0.24059\n",
      "Step [1506/505]: Loss: 0.24051\n",
      "Step [1507/505]: Loss: 0.24043\n",
      "Step [1508/505]: Loss: 0.24027\n",
      "Step [1509/505]: Loss: 0.24012\n",
      "Step [1510/505]: Loss: 0.24000\n",
      "Step [1511/505]: Loss: 0.23984\n",
      "Step [1512/505]: Loss: 0.23970\n",
      "Step [1513/505]: Loss: 0.23954\n",
      "Step [1514/505]: Loss: 0.23938\n",
      "Step [1515/505]: Loss: 0.23950\n",
      "Step [1516/505]: Loss: 0.23934\n",
      "Step [1517/505]: Loss: 0.23918\n",
      "Step [1518/505]: Loss: 0.23903\n",
      "Step [1519/505]: Loss: 0.23925\n",
      "Step [1520/505]: Loss: 0.23910\n",
      "Step [1521/505]: Loss: 0.23914\n",
      "Step [1522/505]: Loss: 0.23898\n",
      "Step [1523/505]: Loss: 0.23883\n",
      "Step [1524/505]: Loss: 0.23867\n",
      "Step [1525/505]: Loss: 0.23851\n",
      "Step [1526/505]: Loss: 0.23836\n",
      "Step [1527/505]: Loss: 0.23894\n",
      "Step [1528/505]: Loss: 0.23879\n",
      "Step [1529/505]: Loss: 0.23864\n",
      "Step [1530/505]: Loss: 0.23894\n",
      "Step [1531/505]: Loss: 0.23879\n",
      "Step [1532/505]: Loss: 0.23863\n",
      "Step [1533/505]: Loss: 0.23854\n",
      "Step [1534/505]: Loss: 0.23840\n",
      "Step [1535/505]: Loss: 0.23824\n",
      "Step [1536/505]: Loss: 0.23809\n",
      "Step [1537/505]: Loss: 0.23793\n",
      "Step [1538/505]: Loss: 0.23778\n",
      "Step [1539/505]: Loss: 0.23769\n",
      "Step [1540/505]: Loss: 0.23824\n",
      "Step [1541/505]: Loss: 0.23809\n",
      "Step [1542/505]: Loss: 0.23794\n",
      "Step [1543/505]: Loss: 0.23793\n",
      "Step [1544/505]: Loss: 0.23778\n",
      "Step [1545/505]: Loss: 0.23870\n",
      "Step [1546/505]: Loss: 0.23854\n",
      "Step [1547/505]: Loss: 0.23839\n",
      "Step [1548/505]: Loss: 0.23846\n",
      "Step [1549/505]: Loss: 0.23831\n",
      "Step [1550/505]: Loss: 0.23816\n",
      "Step [1551/505]: Loss: 0.23800\n",
      "Step [1552/505]: Loss: 0.23785\n",
      "Step [1553/505]: Loss: 0.23859\n",
      "Step [1554/505]: Loss: 0.23844\n",
      "Step [1555/505]: Loss: 0.23828\n",
      "Step [1556/505]: Loss: 0.23816\n",
      "Step [1557/505]: Loss: 0.23803\n",
      "Step [1558/505]: Loss: 0.23788\n",
      "Step [1559/505]: Loss: 0.23773\n",
      "Step [1560/505]: Loss: 0.23758\n",
      "Step [1561/505]: Loss: 0.23743\n",
      "Step [1562/505]: Loss: 0.23728\n",
      "Step [1563/505]: Loss: 0.23738\n",
      "Step [1564/505]: Loss: 0.23723\n",
      "Step [1565/505]: Loss: 0.23708\n",
      "Step [1566/505]: Loss: 0.23693\n",
      "Step [1567/505]: Loss: 0.23684\n",
      "Step [1568/505]: Loss: 0.23669\n",
      "Step [1569/505]: Loss: 0.23654\n",
      "Step [1570/505]: Loss: 0.23645\n",
      "Step [1571/505]: Loss: 0.23630\n",
      "Step [1572/505]: Loss: 0.23632\n",
      "Step [1573/505]: Loss: 0.23641\n",
      "Step [1574/505]: Loss: 0.23626\n",
      "Step [1575/505]: Loss: 0.23612\n",
      "Step [1576/505]: Loss: 0.23597\n",
      "Step [1577/505]: Loss: 0.23582\n",
      "Step [1578/505]: Loss: 0.23605\n",
      "Step [1579/505]: Loss: 0.23590\n",
      "Step [1580/505]: Loss: 0.23576\n",
      "Step [1581/505]: Loss: 0.23668\n",
      "Step [1582/505]: Loss: 0.23653\n",
      "Step [1583/505]: Loss: 0.23640\n",
      "Step [1584/505]: Loss: 0.23625\n",
      "Step [1585/505]: Loss: 0.23610\n",
      "Step [1586/505]: Loss: 0.23668\n",
      "Step [1587/505]: Loss: 0.23661\n",
      "Step [1588/505]: Loss: 0.23646\n",
      "Step [1589/505]: Loss: 0.23633\n",
      "Step [1590/505]: Loss: 0.23703\n",
      "Step [1591/505]: Loss: 0.23694\n",
      "Step [1592/505]: Loss: 0.23679\n",
      "Step [1593/505]: Loss: 0.23727\n",
      "Step [1594/505]: Loss: 0.23738\n",
      "Step [1595/505]: Loss: 0.23733\n",
      "Step [1596/505]: Loss: 0.23719\n",
      "Step [1597/505]: Loss: 0.23710\n",
      "Step [1598/505]: Loss: 0.23695\n",
      "Step [1599/505]: Loss: 0.23680\n",
      "Step [1600/505]: Loss: 0.23666\n",
      "Step [1601/505]: Loss: 0.23670\n",
      "Step [1602/505]: Loss: 0.23657\n",
      "Step [1603/505]: Loss: 0.23642\n",
      "Step [1604/505]: Loss: 0.23627\n",
      "Step [1605/505]: Loss: 0.23613\n",
      "Step [1606/505]: Loss: 0.23631\n",
      "Step [1607/505]: Loss: 0.23634\n",
      "Step [1608/505]: Loss: 0.23621\n",
      "Step [1609/505]: Loss: 0.23607\n",
      "Step [1610/505]: Loss: 0.23592\n",
      "Step [1611/505]: Loss: 0.23595\n",
      "Step [1612/505]: Loss: 0.23595\n",
      "Step [1613/505]: Loss: 0.23580\n",
      "Step [1614/505]: Loss: 0.23702\n",
      "Step [1615/505]: Loss: 0.23688\n",
      "Step [1616/505]: Loss: 0.23673\n",
      "Step [1617/505]: Loss: 0.23660\n",
      "Step [1618/505]: Loss: 0.23646\n",
      "Step [1619/505]: Loss: 0.23632\n",
      "Step [1620/505]: Loss: 0.23618\n",
      "Step [1621/505]: Loss: 0.23632\n",
      "Step [1622/505]: Loss: 0.23618\n",
      "Step [1623/505]: Loss: 0.23603\n",
      "Step [1624/505]: Loss: 0.23710\n",
      "Step [1625/505]: Loss: 0.23696\n",
      "Step [1626/505]: Loss: 0.23681\n",
      "Step [1627/505]: Loss: 0.23670\n",
      "Step [1628/505]: Loss: 0.23656\n",
      "Step [1629/505]: Loss: 0.23642\n",
      "Step [1630/505]: Loss: 0.23627\n",
      "Step [1631/505]: Loss: 0.23615\n",
      "Step [1632/505]: Loss: 0.23680\n",
      "Step [1633/505]: Loss: 0.23674\n",
      "Step [1634/505]: Loss: 0.23711\n",
      "Step [1635/505]: Loss: 0.23696\n",
      "Step [1636/505]: Loss: 0.23685\n",
      "Step [1637/505]: Loss: 0.23671\n",
      "Step [1638/505]: Loss: 0.23657\n",
      "Step [1639/505]: Loss: 0.23658\n",
      "Step [1640/505]: Loss: 0.23644\n",
      "Step [1641/505]: Loss: 0.23630\n",
      "Step [1642/505]: Loss: 0.23622\n",
      "Step [1643/505]: Loss: 0.23608\n",
      "Step [1644/505]: Loss: 0.23623\n",
      "Step [1645/505]: Loss: 0.23626\n",
      "Step [1646/505]: Loss: 0.23703\n",
      "Step [1647/505]: Loss: 0.23758\n",
      "Step [1648/505]: Loss: 0.23748\n",
      "Step [1649/505]: Loss: 0.23753\n",
      "Step [1650/505]: Loss: 0.23739\n",
      "Step [1651/505]: Loss: 0.23739\n",
      "Step [1652/505]: Loss: 0.23732\n",
      "Step [1653/505]: Loss: 0.23747\n",
      "Step [1654/505]: Loss: 0.23733\n",
      "Step [1655/505]: Loss: 0.23727\n",
      "Step [1656/505]: Loss: 0.23713\n",
      "Step [1657/505]: Loss: 0.23721\n",
      "Step [1658/505]: Loss: 0.23773\n",
      "Step [1659/505]: Loss: 0.23761\n",
      "Step [1660/505]: Loss: 0.23843\n",
      "Step [1661/505]: Loss: 0.23834\n",
      "Step [1662/505]: Loss: 0.23822\n",
      "Step [1663/505]: Loss: 0.23810\n",
      "Step [1664/505]: Loss: 0.23849\n",
      "Step [1665/505]: Loss: 0.23834\n",
      "Step [1666/505]: Loss: 0.23904\n",
      "Step [1667/505]: Loss: 0.23891\n",
      "Step [1668/505]: Loss: 0.23876\n",
      "Step [1669/505]: Loss: 0.23906\n",
      "Step [1670/505]: Loss: 0.23938\n",
      "Step [1671/505]: Loss: 0.24002\n",
      "Step [1672/505]: Loss: 0.23988\n",
      "Step [1673/505]: Loss: 0.23973\n",
      "Step [1674/505]: Loss: 0.24016\n",
      "Step [1675/505]: Loss: 0.24034\n",
      "Step [1676/505]: Loss: 0.24042\n",
      "Step [1677/505]: Loss: 0.24028\n",
      "Step [1678/505]: Loss: 0.24014\n",
      "Step [1679/505]: Loss: 0.24001\n",
      "Step [1680/505]: Loss: 0.23987\n",
      "Step [1681/505]: Loss: 0.23977\n",
      "Step [1682/505]: Loss: 0.23963\n",
      "Step [1683/505]: Loss: 0.23981\n",
      "Step [1684/505]: Loss: 0.23967\n",
      "Step [1685/505]: Loss: 0.23953\n",
      "Step [1686/505]: Loss: 0.23939\n",
      "Step [1687/505]: Loss: 0.23925\n",
      "Step [1688/505]: Loss: 0.23911\n",
      "Step [1689/505]: Loss: 0.24048\n",
      "Step [1690/505]: Loss: 0.24163\n",
      "Step [1691/505]: Loss: 0.24149\n",
      "Step [1692/505]: Loss: 0.24135\n",
      "Step [1693/505]: Loss: 0.24205\n",
      "Step [1694/505]: Loss: 0.24191\n",
      "Step [1695/505]: Loss: 0.24177\n",
      "Step [1696/505]: Loss: 0.24204\n",
      "Step [1697/505]: Loss: 0.24228\n",
      "Step [1698/505]: Loss: 0.24213\n",
      "Step [1699/505]: Loss: 0.24200\n",
      "Step [1700/505]: Loss: 0.24186\n",
      "Step [1701/505]: Loss: 0.24179\n",
      "Step [1702/505]: Loss: 0.24207\n",
      "Step [1703/505]: Loss: 0.24192\n",
      "Step [1704/505]: Loss: 0.24184\n",
      "Step [1705/505]: Loss: 0.24170\n",
      "Step [1706/505]: Loss: 0.24214\n",
      "Step [1707/505]: Loss: 0.24208\n",
      "Step [1708/505]: Loss: 0.24193\n",
      "Step [1709/505]: Loss: 0.24179\n",
      "Step [1710/505]: Loss: 0.24165\n",
      "Step [1711/505]: Loss: 0.24152\n",
      "Step [1712/505]: Loss: 0.24142\n",
      "Step [1713/505]: Loss: 0.24127\n",
      "Step [1714/505]: Loss: 0.24131\n",
      "Step [1715/505]: Loss: 0.24129\n",
      "Step [1716/505]: Loss: 0.24115\n",
      "Step [1717/505]: Loss: 0.24143\n",
      "Step [1718/505]: Loss: 0.24253\n",
      "Step [1719/505]: Loss: 0.24239\n",
      "Step [1720/505]: Loss: 0.24297\n",
      "Step [1721/505]: Loss: 0.24285\n",
      "Step [1722/505]: Loss: 0.24329\n",
      "Step [1723/505]: Loss: 0.24315\n",
      "Step [1724/505]: Loss: 0.24301\n",
      "Step [1725/505]: Loss: 0.24287\n",
      "Step [1726/505]: Loss: 0.24274\n",
      "Step [1727/505]: Loss: 0.24264\n",
      "Step [1728/505]: Loss: 0.24250\n",
      "Step [1729/505]: Loss: 0.24274\n",
      "Step [1730/505]: Loss: 0.24361\n",
      "Step [1731/505]: Loss: 0.24407\n",
      "Step [1732/505]: Loss: 0.24393\n",
      "Step [1733/505]: Loss: 0.24380\n",
      "Step [1734/505]: Loss: 0.24366\n",
      "Step [1735/505]: Loss: 0.24414\n",
      "Step [1736/505]: Loss: 0.24487\n",
      "Step [1737/505]: Loss: 0.24474\n",
      "Step [1738/505]: Loss: 0.24484\n",
      "Step [1739/505]: Loss: 0.24486\n",
      "Step [1740/505]: Loss: 0.24472\n",
      "Step [1741/505]: Loss: 0.24458\n",
      "Step [1742/505]: Loss: 0.24543\n",
      "Step [1743/505]: Loss: 0.24596\n",
      "Step [1744/505]: Loss: 0.24688\n",
      "Step [1745/505]: Loss: 0.24674\n",
      "Step [1746/505]: Loss: 0.24660\n",
      "Step [1747/505]: Loss: 0.24646\n",
      "Step [1748/505]: Loss: 0.24632\n",
      "Step [1749/505]: Loss: 0.24620\n",
      "Step [1750/505]: Loss: 0.24607\n",
      "Step [1751/505]: Loss: 0.24593\n",
      "Step [1752/505]: Loss: 0.24579\n",
      "Step [1753/505]: Loss: 0.24566\n",
      "Step [1754/505]: Loss: 0.24552\n",
      "Step [1755/505]: Loss: 0.24538\n",
      "Step [1756/505]: Loss: 0.24696\n",
      "Step [1757/505]: Loss: 0.24774\n",
      "Step [1758/505]: Loss: 0.24760\n",
      "Step [1759/505]: Loss: 0.24769\n",
      "Step [1760/505]: Loss: 0.24851\n",
      "Step [1761/505]: Loss: 0.24907\n",
      "Step [1762/505]: Loss: 0.24897\n",
      "Step [1763/505]: Loss: 0.24883\n",
      "Step [1764/505]: Loss: 0.24869\n",
      "Step [1765/505]: Loss: 0.24892\n",
      "Step [1766/505]: Loss: 0.24879\n",
      "Step [1767/505]: Loss: 0.24964\n",
      "Step [1768/505]: Loss: 0.24950\n",
      "Step [1769/505]: Loss: 0.24936\n",
      "Step [1770/505]: Loss: 0.24923\n",
      "Step [1771/505]: Loss: 0.24927\n",
      "Step [1772/505]: Loss: 0.24925\n",
      "Step [1773/505]: Loss: 0.24911\n",
      "Step [1774/505]: Loss: 0.24897\n",
      "Step [1775/505]: Loss: 0.24924\n",
      "Step [1776/505]: Loss: 0.24910\n",
      "Step [1777/505]: Loss: 0.24896\n",
      "Step [1778/505]: Loss: 0.24905\n",
      "Step [1779/505]: Loss: 0.24931\n",
      "Step [1780/505]: Loss: 0.24978\n",
      "Step [1781/505]: Loss: 0.24964\n",
      "Step [1782/505]: Loss: 0.24963\n",
      "Step [1783/505]: Loss: 0.24949\n",
      "Step [1784/505]: Loss: 0.24935\n",
      "Step [1785/505]: Loss: 0.24921\n",
      "Step [1786/505]: Loss: 0.24908\n",
      "Step [1787/505]: Loss: 0.24894\n",
      "Step [1788/505]: Loss: 0.24880\n",
      "Step [1789/505]: Loss: 0.24867\n",
      "Step [1790/505]: Loss: 0.24853\n",
      "Step [1791/505]: Loss: 0.24839\n",
      "Step [1792/505]: Loss: 0.25053\n",
      "Step [1793/505]: Loss: 0.25047\n",
      "Step [1794/505]: Loss: 0.25033\n",
      "Step [1795/505]: Loss: 0.25042\n",
      "Step [1796/505]: Loss: 0.25028\n",
      "Step [1797/505]: Loss: 0.25014\n",
      "Step [1798/505]: Loss: 0.25081\n",
      "Step [1799/505]: Loss: 0.25088\n",
      "Step [1800/505]: Loss: 0.25074\n",
      "Step [1801/505]: Loss: 0.25066\n",
      "Step [1802/505]: Loss: 0.25122\n",
      "Step [1803/505]: Loss: 0.25108\n",
      "Step [1804/505]: Loss: 0.25172\n",
      "Step [1805/505]: Loss: 0.25158\n",
      "Step [1806/505]: Loss: 0.25163\n",
      "Step [1807/505]: Loss: 0.25149\n",
      "Step [1808/505]: Loss: 0.25221\n",
      "Step [1809/505]: Loss: 0.25248\n",
      "Step [1810/505]: Loss: 0.25244\n",
      "Step [1811/505]: Loss: 0.25232\n",
      "Step [1812/505]: Loss: 0.25334\n",
      "Step [1813/505]: Loss: 0.25336\n",
      "Step [1814/505]: Loss: 0.25370\n",
      "Step [1815/505]: Loss: 0.25357\n",
      "Step [1816/505]: Loss: 0.25392\n",
      "Step [1817/505]: Loss: 0.25414\n",
      "Step [1818/505]: Loss: 0.25400\n",
      "Step [1819/505]: Loss: 0.25394\n",
      "Step [1820/505]: Loss: 0.25380\n",
      "Step [1821/505]: Loss: 0.25377\n",
      "Step [1822/505]: Loss: 0.25363\n",
      "Step [1823/505]: Loss: 0.25387\n",
      "Step [1824/505]: Loss: 0.25373\n",
      "Step [1825/505]: Loss: 0.25359\n",
      "Step [1826/505]: Loss: 0.25441\n",
      "Step [1827/505]: Loss: 0.25519\n",
      "Step [1828/505]: Loss: 0.25591\n",
      "Step [1829/505]: Loss: 0.25599\n",
      "Step [1830/505]: Loss: 0.25585\n",
      "Step [1831/505]: Loss: 0.25571\n",
      "Step [1832/505]: Loss: 0.25564\n",
      "Step [1833/505]: Loss: 0.25554\n",
      "Step [1834/505]: Loss: 0.25541\n",
      "Step [1835/505]: Loss: 0.25550\n",
      "Step [1836/505]: Loss: 0.25538\n",
      "Step [1837/505]: Loss: 0.25525\n",
      "Step [1838/505]: Loss: 0.25511\n",
      "Step [1839/505]: Loss: 0.25497\n",
      "Step [1840/505]: Loss: 0.25484\n",
      "Step [1841/505]: Loss: 0.25470\n",
      "Step [1842/505]: Loss: 0.25541\n",
      "Step [1843/505]: Loss: 0.25527\n",
      "Step [1844/505]: Loss: 0.25514\n",
      "Step [1845/505]: Loss: 0.25528\n",
      "Step [1846/505]: Loss: 0.25557\n",
      "Step [1847/505]: Loss: 0.25546\n",
      "Step [1848/505]: Loss: 0.25619\n",
      "Step [1849/505]: Loss: 0.25617\n",
      "Step [1850/505]: Loss: 0.25604\n",
      "Step [1851/505]: Loss: 0.25590\n",
      "Step [1852/505]: Loss: 0.25577\n",
      "Step [1853/505]: Loss: 0.25563\n",
      "Step [1854/505]: Loss: 0.25549\n",
      "Step [1855/505]: Loss: 0.25640\n",
      "Step [1856/505]: Loss: 0.25637\n",
      "Step [1857/505]: Loss: 0.25626\n",
      "Step [1858/505]: Loss: 0.25718\n",
      "Step [1859/505]: Loss: 0.25704\n",
      "Step [1860/505]: Loss: 0.25754\n",
      "Step [1861/505]: Loss: 0.25740\n",
      "Step [1862/505]: Loss: 0.25727\n",
      "Step [1863/505]: Loss: 0.25714\n",
      "Step [1864/505]: Loss: 0.25702\n",
      "Step [1865/505]: Loss: 0.25758\n",
      "Step [1866/505]: Loss: 0.25745\n",
      "Step [1867/505]: Loss: 0.25731\n",
      "Step [1868/505]: Loss: 0.25717\n",
      "Step [1869/505]: Loss: 0.25703\n",
      "Step [1870/505]: Loss: 0.25690\n",
      "Step [1871/505]: Loss: 0.25676\n",
      "Step [1872/505]: Loss: 0.25662\n",
      "Step [1873/505]: Loss: 0.25649\n",
      "Step [1874/505]: Loss: 0.25686\n",
      "Step [1875/505]: Loss: 0.25693\n",
      "Step [1876/505]: Loss: 0.25680\n",
      "Step [1877/505]: Loss: 0.25687\n",
      "Step [1878/505]: Loss: 0.25673\n",
      "Step [1879/505]: Loss: 0.25660\n",
      "Step [1880/505]: Loss: 0.25646\n",
      "Step [1881/505]: Loss: 0.25633\n",
      "Step [1882/505]: Loss: 0.25619\n",
      "Step [1883/505]: Loss: 0.25606\n",
      "Step [1884/505]: Loss: 0.25592\n",
      "Step [1885/505]: Loss: 0.25683\n",
      "Step [1886/505]: Loss: 0.25670\n",
      "Step [1887/505]: Loss: 0.25657\n",
      "Step [1888/505]: Loss: 0.25643\n",
      "Step [1889/505]: Loss: 0.25630\n",
      "Step [1890/505]: Loss: 0.25617\n",
      "Step [1891/505]: Loss: 0.25603\n",
      "Step [1892/505]: Loss: 0.25590\n",
      "Step [1893/505]: Loss: 0.25589\n",
      "Step [1894/505]: Loss: 0.25630\n",
      "Step [1895/505]: Loss: 0.25695\n",
      "Step [1896/505]: Loss: 0.25681\n",
      "Step [1897/505]: Loss: 0.25668\n",
      "Step [1898/505]: Loss: 0.25654\n",
      "Step [1899/505]: Loss: 0.25641\n",
      "Step [1900/505]: Loss: 0.25637\n",
      "Step [1901/505]: Loss: 0.25646\n",
      "Step [1902/505]: Loss: 0.25653\n",
      "Step [1903/505]: Loss: 0.25646\n",
      "Step [1904/505]: Loss: 0.25658\n",
      "Step [1905/505]: Loss: 0.25645\n",
      "Step [1906/505]: Loss: 0.25638\n",
      "Step [1907/505]: Loss: 0.25625\n",
      "Step [1908/505]: Loss: 0.25612\n",
      "Step [1909/505]: Loss: 0.25598\n",
      "Step [1910/505]: Loss: 0.25643\n",
      "Step [1911/505]: Loss: 0.25630\n",
      "Step [1912/505]: Loss: 0.25625\n",
      "Step [1913/505]: Loss: 0.25634\n",
      "Step [1914/505]: Loss: 0.25623\n",
      "Step [1915/505]: Loss: 0.25610\n",
      "Step [1916/505]: Loss: 0.25597\n",
      "Step [1917/505]: Loss: 0.25588\n",
      "Step [1918/505]: Loss: 0.25585\n",
      "Step [1919/505]: Loss: 0.25571\n",
      "Step [1920/505]: Loss: 0.25558\n",
      "Step [1921/505]: Loss: 0.25545\n",
      "Step [1922/505]: Loss: 0.25532\n",
      "Step [1923/505]: Loss: 0.25518\n",
      "Step [1924/505]: Loss: 0.25571\n",
      "Step [1925/505]: Loss: 0.25557\n",
      "Step [1926/505]: Loss: 0.25607\n",
      "Step [1927/505]: Loss: 0.25594\n",
      "Step [1928/505]: Loss: 0.25581\n",
      "Step [1929/505]: Loss: 0.25626\n",
      "Step [1930/505]: Loss: 0.25612\n",
      "Step [1931/505]: Loss: 0.25599\n",
      "Step [1932/505]: Loss: 0.25586\n",
      "Step [1933/505]: Loss: 0.25582\n",
      "Step [1934/505]: Loss: 0.25571\n",
      "Step [1935/505]: Loss: 0.25608\n",
      "Step [1936/505]: Loss: 0.25595\n",
      "Step [1937/505]: Loss: 0.25585\n",
      "Step [1938/505]: Loss: 0.25572\n",
      "Step [1939/505]: Loss: 0.25559\n",
      "Step [1940/505]: Loss: 0.25550\n",
      "Step [1941/505]: Loss: 0.25546\n",
      "Step [1942/505]: Loss: 0.25548\n",
      "Step [1943/505]: Loss: 0.25535\n",
      "Step [1944/505]: Loss: 0.25553\n",
      "Step [1945/505]: Loss: 0.25579\n",
      "Step [1946/505]: Loss: 0.25566\n",
      "Step [1947/505]: Loss: 0.25562\n",
      "Step [1948/505]: Loss: 0.25570\n",
      "Step [1949/505]: Loss: 0.25557\n",
      "Step [1950/505]: Loss: 0.25544\n",
      "Step [1951/505]: Loss: 0.25531\n",
      "Step [1952/505]: Loss: 0.25520\n",
      "Step [1953/505]: Loss: 0.25507\n",
      "Step [1954/505]: Loss: 0.25500\n",
      "Step [1955/505]: Loss: 0.25487\n",
      "Step [1956/505]: Loss: 0.25489\n",
      "Step [1957/505]: Loss: 0.25476\n",
      "Step [1958/505]: Loss: 0.25489\n",
      "Step [1959/505]: Loss: 0.25591\n",
      "Step [1960/505]: Loss: 0.25578\n",
      "Step [1961/505]: Loss: 0.25568\n",
      "Step [1962/505]: Loss: 0.25555\n",
      "Step [1963/505]: Loss: 0.25542\n",
      "Step [1964/505]: Loss: 0.25529\n",
      "Step [1965/505]: Loss: 0.25558\n",
      "Step [1966/505]: Loss: 0.25595\n",
      "Step [1967/505]: Loss: 0.25582\n",
      "Step [1968/505]: Loss: 0.25569\n",
      "Step [1969/505]: Loss: 0.25557\n",
      "Step [1970/505]: Loss: 0.25544\n",
      "Step [1971/505]: Loss: 0.25531\n",
      "Step [1972/505]: Loss: 0.25518\n",
      "Step [1973/505]: Loss: 0.25505\n",
      "Step [1974/505]: Loss: 0.25492\n",
      "Step [1975/505]: Loss: 0.25486\n",
      "Step [1976/505]: Loss: 0.25473\n",
      "Step [1977/505]: Loss: 0.25487\n",
      "Step [1978/505]: Loss: 0.25563\n",
      "Step [1979/505]: Loss: 0.25551\n",
      "Step [1980/505]: Loss: 0.25538\n",
      "Step [1981/505]: Loss: 0.25525\n",
      "Step [1982/505]: Loss: 0.25513\n",
      "Step [1983/505]: Loss: 0.25500\n",
      "Step [1984/505]: Loss: 0.25487\n",
      "Step [1985/505]: Loss: 0.25485\n",
      "Step [1986/505]: Loss: 0.25472\n",
      "Step [1987/505]: Loss: 0.25465\n",
      "Step [1988/505]: Loss: 0.25452\n",
      "Step [1989/505]: Loss: 0.25440\n",
      "Step [1990/505]: Loss: 0.25427\n",
      "Step [1991/505]: Loss: 0.25436\n",
      "Step [1992/505]: Loss: 0.25423\n",
      "Step [1993/505]: Loss: 0.25412\n",
      "Step [1994/505]: Loss: 0.25415\n",
      "Step [1995/505]: Loss: 0.25402\n",
      "Step [1996/505]: Loss: 0.25389\n",
      "Step [1997/505]: Loss: 0.25435\n",
      "Step [1998/505]: Loss: 0.25422\n",
      "Step [1999/505]: Loss: 0.25410\n",
      "Step [2000/505]: Loss: 0.25397\n",
      "Step [2001/505]: Loss: 0.25384\n",
      "Step [2002/505]: Loss: 0.25372\n",
      "Step [2003/505]: Loss: 0.25359\n",
      "Step [2004/505]: Loss: 0.25370\n",
      "Step [2005/505]: Loss: 0.25358\n",
      "Step [2006/505]: Loss: 0.25345\n",
      "Step [2007/505]: Loss: 0.25333\n",
      "Step [2008/505]: Loss: 0.25320\n",
      "Step [2009/505]: Loss: 0.25307\n",
      "Step [2010/505]: Loss: 0.25295\n",
      "Step [2011/505]: Loss: 0.25282\n",
      "Step [2012/505]: Loss: 0.25270\n",
      "Step [2013/505]: Loss: 0.25258\n",
      "Step [2014/505]: Loss: 0.25250\n",
      "Step [2015/505]: Loss: 0.25237\n",
      "Step [2016/505]: Loss: 0.25225\n",
      "Step [2017/505]: Loss: 0.25281\n",
      "Step [2018/505]: Loss: 0.25292\n",
      "Step [2019/505]: Loss: 0.25319\n",
      "Step [2020/505]: Loss: 0.25307\n",
      "Step [2021/505]: Loss: 0.25295\n"
     ]
    }
   ],
   "source": [
    "bert_clf = BertBinaryClassifier()\n",
    "bert_clf.to(\"cuda\")\n",
    "bert_clf.load_state_dict(torch.load('bert_clf.pth'))\n",
    "bert_clf.eval()\n",
    "\n",
    "results= []\n",
    "tes_loss = 0\n",
    "test_acc = []\n",
    "test_loss = []\n",
    "\n",
    "for step_num, batch_data in enumerate(validation_dataloader):\n",
    "        token_ids, masks, labels = tuple(t for t in batch_data)\n",
    "        #\n",
    "        labels = labels.unsqueeze(1)\n",
    "        labels = labels.float()\n",
    "        #\n",
    "        probas = bert_clf(token_ids, masks)\n",
    "        loss_func = nn.BCELoss()\n",
    "        batch_loss = loss_func(probas, labels)\n",
    "        \n",
    "        for p, l in zip(probas, labels):\n",
    "\n",
    "            if p[0].item()>=0.5:\n",
    "                if l[0] == 1:\n",
    "                    test_acc += [1]\n",
    "                else:\n",
    "                    test_acc += [0]\n",
    "            else:\n",
    "                if l[0] == 0:\n",
    "                    test_acc += [1]\n",
    "                else:\n",
    "                    test_acc += [0]\n",
    "        \n",
    "        tes_loss += batch_loss.item()\n",
    "        test_loss += [tes_loss]\n",
    "        \n",
    "        print(\"Step [%d/%d]: Loss: %.5f\" % (step_num, int(len(validation_data)/16), tes_loss/(step_num+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2022"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the BERT Model for Amazon Reviews Classification\n",
    "import random\n",
    "\n",
    "bert_clf = BertBinaryClassifier()\n",
    "bert_clf.to(\"cuda\")\n",
    "bert_clf.load_state_dict(torch.load('bert_clf.pth'))\n",
    "bert_clf.eval()\n",
    "loss = 0\n",
    "\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, df['label'].tolist(), \n",
    "                                                            random_state=2018, test_size=0.2)\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
    "                                             random_state=2018, test_size=0.2)\n",
    "test_results = []\n",
    "acc = []\n",
    "test_loss = []\n",
    "\n",
    "for _ in tqdm(range(10)):\n",
    "    \n",
    "    args = random.choices(range(len(validation_inputs)), k=500)\n",
    "    \n",
    "    inputs = validation_inputs[args]\n",
    "    labels = validation_labels[args]\n",
    "    masks = validation_masks[args]\n",
    "    \n",
    "    probas = bert_clf(inputs, masks)\n",
    "    \n",
    "    batch_loss = loss_func(probas, labels)\n",
    "    \n",
    "    test_acc = []\n",
    "    \n",
    "    for p, l in tqdm(zip(probas, labels)):\n",
    "\n",
    "    #     print(p[0].item(), l[0].item())\n",
    "        if p[0].item()>=0.5:\n",
    "            if l[0] == 1:\n",
    "                test_acc += [1]\n",
    "            else:\n",
    "                test_acc += [0]\n",
    "        else:\n",
    "            if l[0] == 0:\n",
    "                test_acc += [1]\n",
    "            else:\n",
    "                test_acc += [0]\n",
    "\n",
    "    loss += batch_loss.item()\n",
    "    test_loss += [loss]\n",
    "    acc += [np.mean(test_acc)]\n",
    "    \n",
    "    test_results.append([loss, np.mean(test_acc)])\n",
    "    \n",
    "df = pd.DataFrame(test_results, columns =[\"Test Loss\", \"Test Accuracy\"])\n",
    "df.to_csv(\"bert_clf_test.csv\")\n",
    "\n",
    "print(acc, test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of Accuracy and Loss of Train and Test data\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "filename = 'bert_clf.csv'\n",
    "# scaler = MinMaxScaler()\n",
    "history = pd.read_csv(filename, sep=',')\n",
    "train_acc = history['Train Accuracy']\n",
    "train_loss = history['Train Loss']\n",
    "test_acc = history['Test Accuracy']\n",
    "test_loss = history['Test Loss']\n",
    "\n",
    "epochs = range(10)\n",
    "\n",
    "plt.figure(figsize=(32,10))\n",
    "plt.subplot(2,2,1)\n",
    "plt.plot(epochs, train_acc, 'b', label='Train Accuracy')\n",
    "plt.title('Accuracy')\n",
    "plt.legend(loc=0)\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "plt.plot(epochs, train_loss, 'b', label='Train Loss')\n",
    "plt.title('Loss')\n",
    "plt.legend(loc=0)\n",
    "\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "plt.plot(epochs, test_acc, 'r', label='Test Accuracy')\n",
    "plt.legend(loc=0)\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "plt.plot(epochs, test_loss, 'r', label='Test Loss')\n",
    "plt.legend(loc=0)\n",
    "\n",
    "# plt.savefig('train_history.png')\n",
    "# plt.savefig('test_history.png')\n",
    "# plt.savefig('Accuracy_Loss_Plots.png')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93.85716494048538 151.51954956486765 90.51440583652776 264.04185987462427\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 2 artists>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# bar chart of Training and Testing Accuracy\n",
    "\n",
    "train_ac = sum(train_acc)/len(train_acc) * 100\n",
    "test_ac = sum(test_acc)/len(test_acc) * 100\n",
    "\n",
    "train_los = sum(train_loss)/len(train_loss)\n",
    "test_los = sum(test_loss)/len(test_loss)\n",
    "\n",
    "print(train_ac, train_los, test_ac, test_los)\n",
    "\n",
    "plt.bar(['train', 'test'],[train_ac, test_ac], width = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
